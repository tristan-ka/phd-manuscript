\chapter{Language as a Cognitive Tool to Imagine Goals in Curiosity Driven Exploration: IMAGINE}

\minitoc

\begin{abstract}
Developmental machine learning studies how artificial agents can model the way children learn open-ended repertoires of skills. Such agents need to create and represent goals, select which ones to pursue and learn to achieve them. Recent approaches have considered goal spaces that were either fixed and hand-defined or learned using generative models of states. This limited agents to sample goals within the distribution of known effects. We argue that the ability to imagine out-of-distribution goals is key to enable creative discoveries and open-ended learning. Children do so by leveraging the compositionality of language as a tool to imagine descriptions of outcomes they never experienced before, targeting them as goals during play. We introduce \imagine, an intrinsically motivated deep reinforcement learning architecture that models this ability. Such imaginative agents, like children, benefit from the guidance of a social peer who provides language descriptions. To take advantage of goal imagination, agents must be able to leverage these descriptions to interpret their imagined out-of-distribution goals. This generalization is made possible by modularity: a decomposition between learned goal-achievement reward function and policy relying on deep sets, gated attention and object-centered representations. We introduce the Playground environment and study how this form of goal imagination improves generalization and exploration over agents lacking this capacity. In addition, we identify the properties of goal imagination that enable these results and study the impacts of modularity and social interactions.
\end{abstract}

\section{Motivations}

\begin{figure}[ht!]
  \centering
    \includegraphics[width=\textwidth]{imagine/mainfig_v123.pdf}
  \caption{\textbf{\imagine overview}. In the \textit{Playground} environment, the agent (hand) can move, grasp objects and grow some of them.
  %(animals grow when water or food is placed on them, while plants need water).
  Scenes are generated procedurally with objects of different types, colors and sizes. A social partner provides descriptive feedback (orange), that the agent converts into targetable goals (red bubbles). \label{fig:main}
}
  \vspace{-.5cm}
\end{figure} 


Building autonomous machines that can discover and learn open-ended skill repertoires is a long-standing goal in Artificial Intelligence. In this quest, we can draw inspiration from children development \cite{cangelosi2015developmental}. In particular, children exploration seems to be driven by intrinsically motivated brain processes that trigger spontaneous exploration for the mere purpose of experiencing novelty, surprise or learning progress \cite{gopnik1999scientist, kaplan2007search, kidd2015psychology}. During \textit{exploratory play}, children can also invent and pursue their own problems \cite{chu2020exploratory}.

Algorithmic models of intrinsic motivation were successfully used in developmental robotics \cite{oudeyer2007intrinsic,baldassarre2013intrinsically}, in reinforcement learning \cite{chentanez2005intrinsically,schmidhuber2010formal} and more recently in deep RL \cite{bellemare2016unifying,pathak2017curiosity}. Intrinsically Motivated Goal Exploration Processes (\imgep), in particular, enable agents to sample and pursue their own goals without external rewards \cite{baranes2013active,forestier2016modular,imgep} and can be formulated within the deep RL framework \cite{florensa2017automatic,nair2018visual,curious,pong2019skew,venkattaramanujam2019self,racaniere2019automated}. However, representing goal spaces and goal-achievement functions remains a major difficulty and often requires hand-crafted definitions. Past approaches proposed to learn image-based representations with generative models such as Variational Auto-Encoders \cite{laversanne2018curiosity,nair2018visual}, but were limited to the generation of goals within the distribution of already discovered effects. Moving beyond \textit{within-distribution} goal generation, \textit{out-of-distribution} goal generation could power creative exploration in agents, a challenge that remains to be tackled. 

In this difficult task, children leverage the properties of language to assimilate thousands of years of experience embedded in their culture, in a only a few years \cite{Tomasello1999,Bruner1991}. As they discover language, their goal-driven exploration changes. \citet{Piaget1926} first identified a form of egocentric speech where children narrate their ongoing activities. Later, \citet{Vygotskii1978}
realized that they were generating novel plans and goals by using the expressive generative properties of language. The harder the task, the more children used egocentric speech to plan their behavior \citep[chap. 2]{Vygotskii1978}. Interestingly, this generative capability can push the limits of the real, as illustrated by \citet{Chomsky1957}’s famous example of a sentence that is syntactically correct but semantically original “\textit{Colorless green ideas sleep furiously}”. Language can thus be used to generate out-of-distributions goals by leveraging compositionality to imagine new goals from known ones.

This paper presents \textbf{I}ntrinsic \textbf{M}otivations \textbf{A}nd \textbf{G}oal \textbf{IN}vention for \textbf{E}xploration (\imagine): a learning architecture which leverages natural language (\NL) interactions with a descriptive social partner (\SP) to explore procedurally-generated scenes and interact with objects. \imagine discovers meaningful environment interactions through its own exploration (Figure~\ref{fig:main}a) and episode-level \NL descriptions provided by \SP (\ref{fig:main}b). These descriptions are turned into targetable goals by the agent (\ref{fig:main}c). The agent learns to represent goals by jointly training a language encoder mapping \NL  to goal embeddings and a goal-achievement reward function (\ref{fig:main}d). The latter evaluates whether the current scene satisfies any given goal. These signals (ticks in Figure~\ref{fig:main}d-e) are then used as training signals for policy learning. More importantly, \imagine can invent new goals by composing known ones (\ref{fig:main}f). Its internal goal-achievement function allows it to train autonomously on these imagined goals.

            % % % % % % % % % %
    
\paragraph{Related work.} The idea that language understanding is grounded in one's experience of the world and should not be secluded from the perceptual and motor systems has a long history in Cognitive Science \cite{Glenberg2002,Zwaan05}. This vision was transposed to intelligent systems \cite{steels2006semiotic,mcclelland2019extending}, applied to human-machine interaction \cite{Dominey2005, Madden2010} and recently to deep RL via frameworks such as \textit{BabyAI} \cite{chevalier-boisvert2018babyai}.

In their review of \textit{RL algorithms informed by NL}, \citet{Luketina2019} distinguish between \textit{language-conditional} problems where language is required to solve the task and \textit{language-assisted} problems where language is a supplementary help. In the first category, most works propose instruction-following agents \cite{Branavan2010, Chen2011, bahdanau2018learning, coreyes2018guiding, Jiang2019, Goyal2019, ther}. Although our system is \textit{language-conditioned}, it is not \textit{language-instructed}: it is never given any instruction or reward but sets its own goals and learns its own internal reward function. \citet{bahdanau2018learning} and \citet{fu2018from} also learn a reward function but require extensive expert knowledge (expert dataset and known environment dynamics respectively), whereas our agent uses experience generated by its own exploration.

Language is also particularly well suited for Hindsight Experience Replay \cite{andrychowicz2017hindsight}: descriptions of the current state can be used to relabel trajectories, enabling agents to transfer skills across goals. While previous works used a hard-coded descriptive function \cite{chan2019actrce, Jiang2019} or trained a generative model \cite{ther} to generate goal substitutes, we leverage the learned reward function to scan goal candidates.

To our knowledge, no previous work has considered the use of compositional goal imagination to enable creative exploration of the environment.
The linguistic basis of our goal imagination mechanism is grounded in construction grammar (CG). CG is a usage-based approach that characterizes language acquisition as a trajectory starting with pattern imitation and the discovery of equivalence classes for argument substitution, before evolving towards the recognition and composition of more abstract patterns \cite{tomasello2000item,goldberg2003constructions}. This results in a structured inventory of constructions as form-to-meaning mappings that can be combined to create novel utterances \cite{goldberg2003constructions}. The discovery and substitution of equivalent words in learned schemas is observed directly in studies of child language \cite{tomasello1993twenty, tomasello2000item}.
Computational implementations of this approach have demonstrated its ability to foster generalization \cite{hinaut2013real} and was also used for data augmentation to improve the performance of neural seq2seq models in NLP \cite{andreas2019goodenough}.
 

Imagining goals by composing known ones only works in association with \textit{systematic generalization} \cite{bahdanau2018systematic,hill2019emergent}: generalizations of the type \textit{grow any animal + grasp any plant $\to$ grow any plant}. These were found to emerge in instruction-following agents, including generalizations to new combinations of motor predicates, object colors and shapes \cite{Hermann2017,hill2019emergent, bahdanau2018learning}. Systematic generalization can occur when objects share common attributes (e.g. type, color). 
We directly encode that assumption into our models by representing objects as \textit{single-slot object files} \cite{green2017object}: separate entities characterized by shared attributes. Because all objects have similar features, we introduce a new object-centered inductive bias: object-based modular architectures based on Deep Sets \cite{deepset}.


            % % % % % % % % % %
\paragraph{Contributions.} This paper introduces:
\begin{enumerate}
    \item 
     The concept of imagining new goals using language compositionality to drive exploration.
    \item 
    \imagine: an intrinsically motivated agent that uses goal imagination to explore its environment, discover and master object interactions by leveraging \NL descriptions from a social partner.
    \item 
    Modular policy and reward function with systematic generalization properties enabling \imagine to train on imagined goals. Modularity is based on Deep Sets, gated attention mechanisms and object-centered representations.
    \item 
    \textit{Playground}: a procedurally-generated environment designed to study several types of generalizations (across predicates, attributes, object types and categories).
    \item 
    A study of \imagine investigating: 1) the effects of our goal imagination mechanism on generalization and exploration; 2) the identification of general properties of imagined goals required for any algorithm to have a similar impact; 3) the impact of modularity and 4) social interactions. 
    \end{enumerate}

\section{Playground}

\label{sec:pb_def}
\paragraph{Open-ended learning environment.} We consider a setup where agents evolve in an environment filled with objects and have no prior on the set of possible interactions. An agent decides what and when to learn by setting its own goals, and has no access to external rewards.

%This means that we do not provide any external reward function. 
However, to allow the agent to learn relevant skills, a social partner (\SP) can watch the scene and plays the role of a human caregiver. Following a developmental approach \cite{asada2009cognitive}, we propose a hard-coded surrogate \SP that models important aspects of the developmental processes seen in humans:
\begin{itemize}
    \item 
    At the beginning of each episode, the agent chooses a goal by formulating a sentence. \SP then provides agents with optimal learning opportunities by organizing the scene with: 1) the required objects to reach the goal (not too difficult) 2) procedurally-generated distracting objects (not too easy and providing further discovery opportunities). This constitutes a developmental scaffolding modelling the process of Zone of Proximal Development (ZPD) introduced by Vygotsky to describe infant-parent learning dynamics \cite{Vygotskii1978}. 
    \item 
    At the end of each episode, \SP utters a set of sentences describing achieved and meaningful outcomes (except sentences from a test set). Linguistic guidance given through descriptions are a key component of how parents "teach" language to infants, which contrasts with instruction following (providing a linguistic command and then a reward), that is rarely seen in real parent-child interactions \cite{tomasello2009constructing, bornstein1992maternal}. By default, \SP respects the $3$ following properties: \textit{precision}: descriptions are accurate, \textit{exhaustiveness}: it provides all valid descriptions for each episode and \textit{full-presence}: it is always available. Section~\ref{sec:social_feedbacks} investigates relaxations of the last two assumptions. 
\end{itemize}

Pre-verbal infants are known to acquire object-based representations very early \cite{spelke1992origins,johnson2003development} and, later, to benefit from a simplified parent-child language during language acquisition \cite{mintz2003frequent}. Pursuing a developmental approach \cite{asada2009cognitive}, we assume corresponding object-based representations and a simple grammar. As we aim to design agents that bootstrap creative exploration without prior knowledge of possible interactions or language, we do not consider the use of pre-trained language models.

\paragraph{Evaluation metrics.} This paper investigates how goal imagination can lead agents to efficiently and creatively explore their environment to discover interesting interactions with objects around. In this quest, \SP guides agents towards a set of interesting outcomes by uttering \NL descriptions. Through compositional recombinations of these sentences, goal imagination aims to drive creative exploration, to push agents to discover outcomes beyond the set of outcomes known by \SP. We evaluate this desired behavior by three metrics: 1) the generalization of the policy to new states, using goals from the training set that \SP knows and describes; 2) the generalization of the policy to new language goals, using goals from the testing set unknown to \SP; 3) goal-oriented exploration metrics. These measures assess the quality of the agents' intrinsically motivated exploration. Measures 1) and 2) are also useful to assess the abilities of agents to learn language skills. We measure generalization for each goal as the success rate over $30$ episodes and report $\SR$ the average over goals. We evaluate exploration with the \textit{interesting interaction count} (\itwoc). \itwoc is computed on different sets of interesting interactions: behaviors a human could infer as goal-directed. These sets include the training, testing sets and an extra set containing interactions such as bringing water or food to inanimate objects. $\itwoc_\mathcal{I}$ measures the number of times interactions from $\mathcal{I}$ were observed over the last epoch ($600$ episodes), whether they were targeted or not (see Supplementary Section~\ref{sec:suppl_exploration}). Thus, \itwoc measures the penchant of agents to explore interactions with objects around them. Unless specified  otherwise, we provide means $\mu$ and standard deviations over $10$ seeds and report statistical significance using a two-tail Welch's t-test with null hypothesis $\mu_1=\mu_2$, at level $\alpha=0.05$ (noted by star and circle markers in figures) \cite{colas2019hitchhiker}.

\section{Imagine}

\label{sec:methods}

\subsection{The \textit{Playground} environment}
\label{sec:env}
We argue that the study of new mechanisms requires the use of controlled environments. We thus introduce \textit{Playground}, a simple environment designed to study the impact of goal imagination on exploration and generalization by disentangling it from the problems of perception and fully-blown \NL understanding. The \textit{Playground} environment is a continuous $2$D world, with procedurally-generated scenes containing $\textsc{n}=3$ objects, from $32$ different object types (\textit{e.g. dog, cactus, sofa, water, etc.}), organized into $5$ categories (\textit{animals, furniture, plants, etc}), see Figure~\ref{fig:main}. To our knowledge, it is the first environment that introduces object categories and category-dependent combinatorial dynamics, which allows the study of new types of generalization. We release \textit{Playground} in a separate repository.\footnote{\url{https://github.com/flowersteam/playground\_env}}
%
\paragraph{Agent perception and embodiment.} Agents have access to state vectors describing the scene: the agent's body and the objects. Each object is represented by a set of features describing its type, position, color, size and whether it is grasped. Categories are not explicitly encoded. Objects are made unique by the procedural generation of their color and size. The agent can perform bounded translations in the $2$D plane, grasp and release objects with its gripper. It can make animals and plants grow by bringing them the right supply (food or water for animals, water for plants).  

% % % % % % % % % % % % 
%
%\clearpage
\paragraph{Grammar.}The following grammar generates the descriptions of the $256$ achievable goals ($\G^\text{A}$): 
\begin{enumerate}
    \item 
    Go: <\textit{go} + \bi{zone}>  \textit{(e.g. go bottom left) }
    \item 
    Grasp: < \textit{grasp} + \textit{any} + \bi{color} + \textit{thing}>  \textit{(e.g. grasp any blue thing)} OR \\
    <\textit{grasp} + \bi{color} $\cup$ \{\textit{any}\}  + \textbf{\textit{object type $\cup$ object category}}> \textit{(e.g. grasp red cat)}
    \item 
    Grow: <\textit{grow} + \textit{any} + \bi{color} + \textit{thing}> \textit{(e.g. grow any red thing)} OR \\
    <\textit{grow} + \bi{color} $\cup$ \{\textit{any}\} + \bi{living thing} $\cup$ \{\textit{living\_thing, animal, plant}\}> \textit{(e.g. grow green animal)}
\end{enumerate} 

\textbf{Bold} and \{ \} are sets of words while \textit{italics} are specific words. 
%\bi{Zone} refers to areas of the scene (e.g. \textit{left}), \bi{object type} refers to a type (e.g. \textit{parrot}) and \bi{object category} to one of $5$ object categories (e.g. \textit{living\_thing}). \bi{living thing} refers to any plant or animal word, \bi{color} is one of \textit{blue, green, red} and \textit{any} refers to any color, or any object. 
The grammar is structured around the $3$ predicates \textit{go}, \textit{grasp} and \textit{grow}. Objects can be referred to by a combination of their color and either their object name or category, or simply by one of these.
%\textit{grasp any red furniture}, \textit{grow any living\_thing} are additional examples of goals.
The set of achievable goals is partitioned into \textit{training} $(\G^\train)$ and \textit{testing}  $(\G^\test)$. $\G^\test$ maximizes the compound divergence with a null atom divergence with respect to $\G^\train$: testing sentences (compounds) are out of the distribution of $\G^\train$ sentences, but their words (atoms) belong to the distribution of words in $\G^\train$ \cite{keysers2019measuring}. \SP only provides descriptions from $\G^\train$. We limit the set of goals to better control the complexity of our environment and enable a careful study of the generalization properties. Supplementary Section~\ref{sec:suppl_env_descr} provides more details about the environment, the grammar and \SP as well as the pseudo-code of our learning architecture.

% % % % % % % % % % % % 

\subsection{The \imagine Architecture}
\label{sec:architecture}
\imagine agents build a repertoire of goals and train two internal models: 1) a goal-achievement reward function $\mathcal{R}$ to predict whether a given description matches a behavioral trajectory; 2) a policy $\pi$ to achieve behavioral trajectories matching descriptions. The architecture is presented in Figure~\ref{fig:architecture} and follows this logic:  
\begin{enumerate}
    \item The \textit{Goal Generator} samples a target goal $g_{\text{target}}$ from known and imagined goals $(\G_\text{known} \cup \G_\text{im})$.
    \item The agent (\textit{RL Agent}) interacts with the environment using its policy $\pi$ conditioned on $g_\text{target}$.
    \item State-action trajectories are stored in a replay buffer \textit{mem$(\pi)$}.
    \item \SP's descriptions of the last state are considered as potential goals $\G_{\text{\SP}}(\mathbf{s}_T)~=~\mathcal{D}_{\text{\SP}}(\textbf{s}_T)$.
    \item \textit{mem$(\mathcal{R})$} stores positive pairs $(\mathbf{s}_T,~ \G_{\text{\SP}}(\mathbf{s}_T))$ and infers negative pairs $(\mathbf{s}_T,~\G_\text{known} \setminus \G_{\text{\SP}}(\mathbf{s}_T))$.
    \item The agent then updates:
    \begin{itemize}
        \item \textit{Goal Gen.}: $\G_\text{known}~\gets~\G_\text{known} \cup \G_{\text{\SP}}(\textbf{s}_T)$ and  $\G_{im}~\gets~\text{Imagination}(\G_\text{known})$.
        \item \textit{Language Encoder} $(L_e)$ and \textit{Reward Function} $(\mathcal{R})$ are updated using data from \textit{mem$(\mathcal{R})$}.
        \item \textit{RL agent}: We sample a batch of state-action transitions $(\mathbf{s},~\mathbf{a},~\mathbf{s}')$ from \textit{mem$(\pi)$}. Then, we use \textit{Hindsight Replay} and $\mathcal{R}$ to bias the selection of substitute goals to train on $(g_\text{s})$ and compute the associated rewards $(\mathbf{s},~\mathbf{a},~\mathbf{s'},~g_\text{s},~r)$. Substituted goals $g_s$ can be known or imagined goals. Finally, the policy and critic are trained via RL.
    \end{itemize}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%z
\begin{figure}[!h]
    \centering
    % \vspace{-0.25cm}
     \includegraphics[width=.7\textwidth]{imagine/imagine_archi.pdf}
    
     \caption{\textbf{\imagine architecture.} Colored boxes show the different modules of \imagine. Lines represent update signals (dashed) and function outputs (plain). The language encoder $L_e$ is shared.}  
    \vspace{-0.25cm}
\label{fig:architecture}
\end{figure}

\paragraph{Goal generator.} It is a generative model of \NL  goals. It generates target goals $g_\text{target}$ for data collection and substitutes goals $g_\text{s}$ for hindsight replay. When goal imagination is disabled, the goal generator samples uniformly from the set of known goals $\G_\text{known}$, sampling random vectors if empty. When enabled, it samples with equal probability from $\G_\text{known}$ and $\G_\text{im}$ (set of imagined goals). $\G_\text{im}$ is generated using a mechanism grounded in construction grammar that leverages the compositionality of language to imagine new goals from $\G_\text{known}$. The heuristic consists in computing sets of \textit{equivalent words}: words that appear in two sentences that only differ by one word. For example, from \textit{grasp red lion} and \textit{grow red lion}, \textit{grasp} and \textit{grow} can be considered \textit{equivalent} and from \textit{grasp green tree} one can imagine a new goal \textit{grow green tree} (see Figure~\ref{fig:main}f). Imagined goals do not include known goals. Among them, some are meaningless, some are syntactically correct but infeasible (e.g. \textit{grow red lamp}) and some belong to $\G^\text{test}$, or even to $\G^\train$ before they are encountered by the agent and described by \SP. The pseudo-code and all imaginable goals are provided in Supplementary Section~\ref{sec:suppl_goal_imagination}. 


% % % % % % % % % % % % 

\paragraph{Language encoder.} The language encoder $(L_e)$ embeds \NL  goals $(L_e:\G^\text{\NL} \to \mathbb{R}^{100})$ using an LSTM \cite{hochreiter1997lstm} trained jointly with the reward function.  $L_e$ acts as a goal translator, turning the goal-achievement reward function, policy and critic into language-conditioned functions.

% % % % % % % % % % % % 

\paragraph{Object-centered modular architectures.} The goal-achievement reward function, policy and critic leverage novel \textit{modular-attention} (\MA) architectures based on Deep Sets \cite{deepset}, gated attention mechanisms \cite{chaplot2017gatedattention} and object-centered representations. The idea is to ensure efficient skill transfer between objects, no matter their position in the state vector. This is done through the combined use of a shared neural network that encodes object-specific features and a permutation-invariant function to aggregate the resulting latent encodings. The shared network independently encodes, for each object, an affordance between this object (object observations), the agent (body observations) and its current goal. The goal embedding, generated by $L_e$, is first cast into an attention vector in $[0,~1]$, then fused with the concatenation of object and body features via an Hadamard product (gated-attention \cite{chaplot2017gatedattention}). The resulting object-specific encodings are aggregated by a permutation-invariant function and mapped to the desired output via a final network (e.g. into actions or action-values). Supplementary Section~\ref{sec:suppl_archi} provides visual representations.

% % % % % % % % % % % % 
\paragraph{Reward function.} Learning a goal-achievement reward function $(\mathcal{R})$ is framed as binary classification: $\mathcal{R}(\mathbf{s}, \mathbf{g}):~\mathcal{S} \times \mathbb{R}^{100} \to \{0, 1\}$. We use the \MA architecture with attention vectors $\balpha^g$, a shared network $\textsc{nn}^\mathcal{R}$ with output size $1$ and a logical OR aggregation. $\textsc{nn}^\mathcal{R}$ computes object-dependent rewards $r_{i}$ in $[0,1]$ from the object-specific inputs and the goal embedding. The final binary reward is computed by \textsc{nn}$^\textsc{or}$ which outputs $1$ whenever $ \exists j:~r_{j}~>~0.5$. We pre-trained a neural-network-based \textsc{or} function to enable end-to-end training with back-propagation. The overall function is:

\begin{equation*}
   \mathcal{R}(\mathbf{s}, g)~=~\textsc{nn}^\textsc{OR}([\textsc{nn}^\mathcal{R}(\mathbf{s}_{obj(i)} \odot \balpha^g)]_{i\in[1..N]})
\end{equation*}


\textit{Data.} 
Interacting with the environment and \SP, the agent builds a set of entries $[\mathbf{s}_T,~ g,~ r]$ with $g~\in~\G_\text{known}$ where $r \in \{0,~1\}$ rewards the achievement of $g$ in state $\mathbf{s}_T$: $r~=~1$ if $g \in \G_\text{\SP}(\mathbf{s}_T)$ and $0$ otherwise. $L_e$ and $\mathcal{R}$ are periodically updated jointly by back-propagation on this dataset.


% % % % % % % % % % % % 

\paragraph{Multi-goal RL agent.} Our agent is controlled by a goal-conditioned policy $\pi$ \cite{schaul2015universal} based on the \MA architecture (see Supplementary Figure~\ref{fig:archi_MA}). It uses an attention vector $\bbeta^g$, a shared network \textsc{nn}$^\pi$, a sum aggregation and a mapper \textsc{nn}$^\text{a}$ that outputs the actions. Similarly, the critic produces action-values via $\bgamma^g$, \textsc{nn}$^Q$ and $\textsc{nn}^\text{a-v}$ respectively: 

\begin{align*}
    \pi(\mathbf{s}, g)~&=~\textsc{nn}^\text{a}(\sum_{i \in [1..N]}\textsc{nn}^\pi(\mathbf{s}_{obj(i)} \odot \bbeta^g)) 
    \qquad 
    Q(\mathbf{s}, \mathbf{a}, g)~=~\textsc{nn}^\text{a-v}(\sum_{i \in [1..N]}\textsc{nn}^Q([\mathbf{s}_{obj(i)},~\mathbf{a}] \odot \bgamma^g)).
\end{align*}

Both are trained using \ddpg \cite{lillicrap2015continuous}, although any other off-policy algorithm can be used. As detailed in Supplementary Section~\ref{sec:suppl_reward}, our agent uses a form of Hindsight Experience Replay \cite{andrychowicz2017hindsight}.


\section{Experiments}

This section first showcases the impact of goal imagination on exploration and generalization (Section~\ref{sec:res_imag_exp}). For a more complete picture, we analyze other goal imagination mechanisms and investigate the properties enabling these effects (Section~\ref{sec:res_im_properties}). Finally, we show that our modular architectures are crucial to a successful goal imagination (Section~\ref{sec:res_archi}) and discuss more realistic interactions with \SP (Section~\ref{sec:social_feedbacks}). \imagine agents achieve near perfect generalizations to new states (training set of goals): $\SR~=~0.95\pm0.05$. We thus focus on language generalization and exploration. Supplementary Sections~\ref{sec:supp_focus_gene} to \ref{sec:suppl_visu} provide additional results and insights organized by theme (Generalization, Exploration, Goal Imagination, Architectures, Reward Function and Visualizations).

% % % % % % %% % % % % % %% % % % % % %
\subsection{How does Goal Imagination Impact Generalization and Exploration?}
\label{sec:res_imag_exp}

\paragraph{Global generalization performance.} 
Figure~\ref{fig:goal_invention_all} shows $\SR$ on the set of testing goals, when the agent starts imagining new goals early (after $6\cdot10^3$ episodes), half-way (after $48\cdot10^3$ episodes) or when not allowed to do so. Imagining goals leads to significant improvements in generalization.

\begin{figure}[!h]
% \vspace{-.cm}
    \centering
    \begin{tabular}{ccc}
		\includegraphics[width=0.323\textwidth]{imagine/GOAL_INVENTION_gene_compressed.pdf} & \includegraphics[width=0.323\textwidth]{imagine/GOAL_INVENTION_adaptation_from_epoch_80_compressed.pdf} & 
		\includegraphics[width=0.323\textwidth]{imagine/GOAL_INVENTION_exploration_count_reward_test_set_compressed.pdf}
		\\
		(a) & (b) & (c)
    \end{tabular}
    \caption{\textbf{Goal imagination drives exploration and generalization.} Vertical dashed lines mark the onset of goal imagination. (a) $\SR$ on testing set. (b) Behavioral adaptation, empirical probabilities that the agent brings supplies to a plant when trying to grow it. (c) \itwoc computed on the testing set. Stars indicate significance (a and c are tested against \textit{never}).}
    \label{fig:adaptation}
    \vspace{-.3cm}
\end{figure} 



\paragraph{A particular generalization: growing plants.} Agents learn to grow animals from \SP's descriptions, but are never told they could grow plants. When evaluated offline on the \textit{growing-plants} goals before goal imagination, agents' policies perform a sensible zero-shot generalization and bring them water or food with equal probability, as they would do for animals (Figure~\ref{fig:adaptation}, left). As they start to imagine and target these goals, their behavior adapts (Figure~\ref{fig:adaptation}, right). If the reward function shows good zero-shot abilities, it only provides positive rewards when the agent brings water. The policy slowly adapts to this internal reward signal and pushes agents to bring more water. We call this phenomenon \textit{behavioral adaptation}. Supplementary Section~\ref{sec:supp_focus_gene} details the generalization abilities of \imagine for $5$ different types of generalizations involving predicates, attributes and categories.



\paragraph{Exploration.} 
Figure~\ref{fig:goal_invention_explo} presents the \itwoc metric computed on the set of interactions related to $\G^\test$ and demonstrates the exploration boost triggered by goal imagination. Supplementary Section~\ref{sec:suppl_exploration} presents other \itwoc metrics computed on additional interactions sets.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\subsection{What If We Used Other Goal Imagination Mechanisms?}
\label{sec:res_im_properties}

\paragraph{Properties of imagined goals.} We propose to characterize goal imagination mechanisms by two properties: 1) \textit{Coverage}: the fraction of $\mathcal{G^\text{test}}$ found in $\mathcal{G_\text{im}}$ and 2) \textit{Precision}: the fraction of the imagined goals that are achievable. We compare our goal imagination mechanism based on the construction grammar heuristic (\CGH) to variants characterized by 1) lower coverage; 2) lower precision; 3) perfect coverage and precision (oracle); 4) random goal imagination baseline (random sequences of words from $\G^\train$ leading to near null coverage and precision). These measures are computed at the end of experiments, when all goals from $\G^\train$ have been discovered (Figure~\ref{fig:goal_invention_properties_explo}a). 




Figure~\ref{fig:goal_invention_properties_explo}b shows that \CGH achieves a generalization performance on par with the oracle. Reducing the coverage of the goal imagination mechanism still brings significant improvements in generalization. Supplementary Section~\ref{sec:suppl_goal_imagination} shows, for the \textit{Low Coverage} condition, that the generalization performance on the testing goals that were imagined is not statistically different from the performance on similar testing goals that could have been imagined but were not. This implies that the generalization for imagined goals also benefits similar non-imagined goals from $\G^\text{test}$. Finally, reducing the precision of imagined goals (gray curve) seems to impede generalization (no significant difference with the \textit{no imagination} baseline). Figure~\ref{fig:goal_invention_properties_explo}c shows that all goal imagination heuristics enable a significant exploration boost. The random goal baseline acts as a control condition. It demonstrates that the generalization boost is not due to a mere effect of network regularization introduced by adding random goals (no significant effect w.r.t. the \textit{no imagination} baseline). In the same spirit, we also ran a control using random goal embeddings, which did not produce any significant effects.

\begin{figure}[!h]
\begin{minipage}{.3\textwidth} %
    \centering
    \tiny
    \vspace{.2cm}
    \begin{tabular}{l|cc}
         & \textbf{Cov.} & \textbf{Pre.}\\
         \hline
        \textbf{\CGH} & 0.87 & 0.45  \\
        \textbf{Oracle} &  1 & 1 \\
        \textbf{Low Cov.} & 0.44 & 0.45 \\
        \textbf{Low Pre.} & 0.87 & 0.30 \\
        \textbf{Random G.} & $\approx$0 & $\approx$0 \\
    \end{tabular}
    \\
    \vspace{1.05cm}
    \textit{ }\\
    (a)
\end{minipage}
\hfill
\begin{minipage}{.34\textwidth} %
    \centering
      \includegraphics[width=0.98\linewidth]{imagine/GOAL_INVENTION_gene_properties.pdf}\\
      (b)
\end{minipage} %
\hfill
\begin{minipage}{.34\textwidth} %
    \centering
    \includegraphics[width=0.98\linewidth]{imagine/explo_test.pdf}\\
    (c)
\end{minipage} %

\caption{\textbf{Goal imagination properties.} (a) Coverage and precision of different goal imagination heuristics. (b) $\SR$ on testing set. (c) \itwoc on $\G^\test$. We report \textit{sem} (standard error of the mean) instead of \textit{std} to improve readability. Stars indicate significant differences w.r.t the \textit{no imagination} condition.\label{fig:goal_invention_properties_explo}}
\end{figure}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\subsection{How Does Modularity Interact with Goal Imagination?}
\label{sec:res_archi}

\begin{table}
    \caption{Policy architectures performance. 
    $\SR_\text{test}$ at convergence.}
    \label{tab:archi_comparison}
    \centering
    \begin{tabular}{l|cc}
    & \MA$^\textbf{*}$ & \FA \\
    \hline    
    Im. & $0.76 \pm 0.1$ &  $0.15 \pm 0.05$ \\
    No Im. & $0.51 \pm 0.1$  & $0.17 \pm 0.04$ \\
    \hline
    p-val & $4.8$e-5 & 0.66
    \end{tabular}
\end{table}
We compared \MA to flat architectures (\FA) that consider the whole scene at once. As the use of \FA for the reward function showed poor performance on $\G^\train$, Table~\ref{tab:archi_comparison} only compares the use of \MA and \FA for the policy. \MA shows stronger generalization and is the only architecture allowing an additional boost with goal imagination. Only \MA policy architectures can leverage the novel reward signals coming from imagined goals and turn them into \textit{behavioral adaptation}. Supplementary Section~\ref{sec:suppl_archi} provides additional details.

%We hypothesized that the gains introduced by the ability to imagine goals would require good generalization abilities from the start. Indeed, agents need to generalize the meaning of imagined goals from known ones, for these goals to drive behavior. We compared our \MA architectures to a concurrent flat architecture (\FA) that considers the whole scene at once. Table~\ref{tab:archi_comparison} presents the $F_1$ scores of the associated reward function. Lower generalization in \FA prevents the generalization boost, only seen in \MA (see full curves in Supplementary Section~\ref{sec:suppl_archi}).


\subsection{Can We Use More Realistic Feedbacks?}
%
\label{sec:social_feedbacks}
\begin{figure}
\centering
\vspace{-.3cm}
     \includegraphics[width=0.5\textwidth]{imagine/SOCIAL_PARTNER_sr_test_compressed.pdf}
     \vspace{-0.5cm}
        \caption{\textbf{Influence of social feedbacks.} $\SR$ on $\G^\text{test}$ for different social strategies. Stars indicate significant differences w.r.t. \textit{ex:1 no imag.}. sem plotted, 5 seeds.}
      \label{fig:social}
\end{figure}
We study the relaxation of the \textit{full-presence} and \textit{exhaustiveness} assumptions of \SP. We first relax \textit{full-presence} while keeping \textit{exhaustiveness} (blue, yellow and purple curves). When \SP has a 10\% chance of being present (yellow), imaginative agents show generalization performance on par with the unimaginative agents trained in a full-presence setting (green), see Figure~\ref{fig:social}). However, when the same amount of feedback is concentrated in the first 10\% episodes (purple), goal imagination enables significant improvements in generalization (w.r.t. green).  This is reminiscent of children who require less and less attention as they grow into adulthood and is consistent with \citet{chan2019actrce}. Relaxing \textit{exhaustiveness}, \SP only provides one positive and one negative description every episode (red) or in 50\% of the episodes (gray). Then, generalization performance matches the one of unimaginative agents in the exhaustive setting (green). 

\section{Discussion and Conclusion}
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\imagine is a learning architecture that enables autonomous learning by leveraging \NL interactions with a social partner. As other algorithms from the \imgep family, \imagine sets its own goals and builds behavioral repertoires without external rewards. As such, it is distinct from traditional instruction-following RL agents. This is done through the joint training of a language encoder for goal representation and a goal-achievement reward function to generate internal rewards. Our proposed modular architectures with gated-attention enable efficient out-of-distribution generalization of the reward function and policy. The ability to imagine new goals by composing known ones leads to further improvements over initial generalization abilities and fosters exploration beyond the set of interactions relevant to \SP. Our agent even tries to grow pieces of furniture with supplies, a behavior that can echo the way a child may try to feed his doll. 

\imagine does not need externally-provided rewards but learns which behaviors are \textit{interesting} from language-based interactions with \SP. In contrast with hand-crafted reward functions, \NL descriptions provide an easy way to guide machines towards relevant interactions. \textit{A posteriori} counterfactual feedback is easier to communicate for humans, especially when possible effects are unknown and, thus, the set of possible instructions is undefined. Hindsight learning also greatly benefits from such counterfactual feedback and improves sample efficiency.
Attention mechanisms further extend the interpretability of the agent's learning by mapping language to attentional scaling factors (see Supplementary Figure~\ref{fig:att}). In addition,  Section~\ref{sec:social_feedbacks} shows that agents can learn to achieve goals from a relatively small number of descriptions, paving the way towards human-provided descriptions. 

\textit{Playground} is a tool that we hope will enable the community to further study under-explored descriptive setups with rich combinatorial dynamics, as well as goal imagination. It is designed for the study of goal imagination and combinatorial generalization. Compared to existing environments \citep{Hermann2017,chevalier-boisvert2018babyai,chan2019actrce}, we allow the use of descriptive feedback, introduce the notion of object categories and category-dependent object interactions (\textit{Grow} refer to different modalities for \textit{plants} or \textit{animals}). Playground can easily be extended by adding objects, attributes, category- or object-type-dependent dynamics.

\imagine could be combined with unsupervised multi-object representation learning algorithms \cite{burgess2019monet, greff2019multi} to work directly from pixels, practically enforcing object-centered representations. The resulting algorithm would still be different from goal-as-state approaches \cite{nair2018visual,pong2019skew,nair2019contextual}. Supplementary Section~\ref{sec:suppl_discu} discusses the relevance of comparing \imagine to these works. Some tasks involve instruction-based navigation in visual environments that do not explictly represent objects \citep{nguyen2019vision,shridhar2020alfred}. Here, also, imagining new instructions from known ones could improve exploration and generalization. Finally, we believe \imagine could provide interesting extensions in hierarchical settings, like in \citet{Jiang2019}, with novel goal imagination boosting low-level exploration.

\paragraph{Future work.} A more complex language could be introduced, for example, by considering object relationships (e.g. \textit{Grasp any X left of Y}), see \cite{karch2020deep} for a preliminary experiment in this direction. While the use of pre-trained language models \cite{radford2019language} does not follow our developmental approach, it would be interesting to study how they would interact with goal imagination. Because \CGH performs well in our setup with a medium precision ($0.45$) and because similar mechanisms were successfully used for data augmentation in complex NLP tasks \cite{andreas2019goodenough}, we believe our goal imagination heuristic could scale to more realistic language. 

We could reduce the burden on \SP by considering unreliable feedbacks (lower precision), or by conditioning goal generation on the initial scene (e.g. using mechanisms from \citet{ther}). One could also add new interaction modalities by letting \SP make demonstrations, propose goals or guide the agent's attention. Our modular architectures, because they are set functions, could also directly be used to consider variable numbers of objects. Finally, we could use off-policy learning \cite{fujimoto2018off} to reinterpret past experience in the light of new imagined goals without any additional environment interactions.


\paragraph{Links.} Demonstration videos are available at \url{https://sites.google.com/view/imagine-drl}. The source code of playground environment can be found at \url{https://github.com/flowersteam/playground\_env} and the source code of the \imagine architecture \url{https://github.com/flowersteam/Imagine}.




