\chapter{Background: Standard AI Paradigms}
\label{chap:backgroud}
\adjustmtc
\minitoc

Our contributions bridge standard \ai paradigms and developmental psychology to investigate two fundamental research questions  (1) the language acquisition problem (self-organisation of cultural conventions) and (2) the open-ended skill acquisition problem (self-organisation of trajectories). In this chapter, we will first present standard \ai problems and their associated families of algorithmic solutions before getting into the specifications of the two problems we investigate.

\section{Reinforcement Learning}

\label{sec:background_rl}

\paragraph{Problem}

In a Reinforcement Learning problem, an agent learns to perform sequences of actions in an environment by maximizing some notion of cumulative reward~\citep{sutton2018reinforcement}.  The agent interacts with the environment in the form of a temporal sequence unfolding from time $t=0$ to time $t=T$, $T$ being the episode horizon and representing the lifetime of the agent (potentially variable or infinite). \rl problems are commonly framed as Markov Decision Processes (\mdps).
\begin{tcolorbox}
\small
\paragraph{Definition}
\gls{Markov Decision Process} (\mdp):
\begin{equation}
	\m{M}\,=\,\{\m{S},\,\m{A},\,\m{T},\,\rho_0,\,R\}
	\label{eq:standard_mdp}	
\end{equation}
where $\m{S}$ and $\m{A}$ are respectively the state and action spaces, $\m{T}$ is the transition function that dictates how actions impact the world (lead to the next state), $\rho_0$ is the initial state distribution and $R$ is the reward function. 
\end{tcolorbox}
%\textbf{Interactions. }  
At the beginning of an episode, the agent starts in the initial state $s_0 \sim \rho_0(\mathcal{S})$. At each time step the agents takes action $a_t \in \mathcal{A}$ and observes the next state $s'=s_{t+1} \in \m{S}$ and the reward $r_{t+1}=R(s_t,a_t)$. A diagram of interaction is given in \fig{fig:rl_interacvtions}.
\begin{figure}[!h]
\centering
\includegraphics[width=0.3\textwidth]{background/rl_interactions.pdf}	
\caption{Interactions in a \rl loop}
\label{fig:rl_interacvtions}
\end{figure}
The transition function $\m{T}$ gives the distribution of the following states from the current state and action: $\m{T}=P_E(.|s, a)$ with $P_E$ being the (potentially stochastic) dynamics of the environment. In an \mdp, the transition function must respect the \textit{Markov property}: a future state ($s'$) must only depend on the current state ($s$) and not on its predecessor, i.e. the transition function is memoryless. 
\begin{equation}
 P_E(s_{t+1}|s_t,a_t) = P_E(s_{t+1}|s_0,\dots,s_t,a_t)	
\end{equation}

In a \rl problem, the behavior of the agent is expressed as a \textit{policy} $\pi : \m{S} \rightarrow \m{A}$ that predicts the next action $a$ based on the current state $s$. This policy can be stochastic ${a_t\sim\pi(.|s_t)}$ or deterministic $a_t=\bar{\pi}(s_t)$. When agents interact in an environment, they produce \textit{trajectories}. A trajectory is a sequence of states and actions $\tau=(s_0,a_0,\dots,s_T,a_T)$. When both the dynamics of the environment and the policy of the agent is stochastic, the probability of a trajectory is:
\begin{equation}
P(\tau|\pi)=\rho_0(s_0)\prod_{t=0}^{T-1} P_E(s_{t+1}|s_t,a_t)\pi(a_t|s_t)	
\label{eq:traj_dist}
\end{equation}
The objective of the agent is to maximize the cumulative reward computed over trajectories ($R^{tot}$). When computing the aggregation of rewards,  we often introduce discounting and give smaller weights to delayed rewards. The return of a trajectory is therefore:
\begin{equation}
R^{tot}(\tau) = \sum_{t=0}^{T} \gamma^{t}R(s_t,a_t)
\end{equation}
with $\gamma \in ]0,1]$ being a constant discount factor. We call the optimal policy $\pi^*$, the behavior that maximizes the expected return:
\begin{equation}
	\pi^*=\argmax_\pi \expect{\tau\sim\pi}[R^{tot}(\tau)] = \argmax_\pi \expect{(a_t\sim\pi, s_t\sim P_E)}\left[\sum_{t=0}^{T} \gamma^{t}R(s_t,a_t)\right]
\end{equation}
The reward function plays therefore a crucial role in a \rl problem as its maximization will directly shape the behavior of the agent.

\paragraph{Value Functions}

Most \rl algorithms rely on the definition of \textit{value} and \textit{action-value} functions:

\begin{tcolorbox}
\small
\paragraph{Definitions}
\begin{itemize}[noitemsep]
	 \item The \gls{Value Function} $V_\pi(s)$ of a policy $\pi$ gives the expected return of a trajectory starting from $s$ and following $\pi$. 
	 \item The \gls{Action-value Function} $Q_\pi(s,a)$ is the expected return of the trajectory taking action $a$ from state $s$ before following $\pi$ from the next state $s'$. 
\end{itemize}
\end{tcolorbox}
Action-value functions are powerful because they allow us to instantly assess the quality of a situation without waiting for the end of the trajectory. The value and action-value function obey the Bellman expectation equations~\citep{sutton1998intra}, a recursive definition that states that the value of a certain state (when following policy $\pi$) is equal to the sum of the instantaneous reward and the value from the next state.
\begin{equation}
\left\{
\begin{split}
	V_\pi(s) & = \underset{(a\sim\pi, s'\sim P_E)}{\mathbb{E}}\left[R(s,a) + \gamma V_\pi(s')\right]  \\
	Q_\pi(s,a) &= \underset{s'\sim P_E}{\mathbb{E}}\left[ R(s,a) + \gamma \underset{a' \sim \pi}{\mathbb{E}}[Q_\pi(s',a')]\right]
\end{split}
\right.	
\end{equation}
The value and action-value functions also follow the Bellman optimality equation where expectations over actions are replaced by max operators.
\begin{equation}
\left\{
\begin{split}
	V^*(s) & = \max_a\underset{s'\sim P_E}{\mathbb{E}}\left[R(s,a) + \gamma V^*(s')\right]  \\
	Q^*(s,a) &= \underset{s'\sim P_E}{\mathbb{E}}\left[ R(s,a) + \gamma \max_{a'}[Q^*(s',a')]\right]
\end{split}
\right.	
\end{equation}
Acting greedily with respect to the optimal action-value function gives the optimal policy:
\begin{equation}
\pi^*(s) = \argmax_a Q^*(s,a)	
\end{equation}


Computing $Q^*$ is therefore a way to solve a \rl problem. When agents have access to perfect knowledge of the dynamic of the environment ($P_E$) and when the dimensionality of $\m{S}$ and $\m{A}$ is small, they can do planning to find the optimal action-value function via Dynamic Programming~\citep{bellman1966dynamic} for instance. Planning approaches that leverage the transition function of the environments are called \textit{model-based} \rl algorithms. They are opposed to \textit{model-free} \rl algorithms that do not use $P_E$ but interact directly with a simulator (with transition function $P_S$).

Because the present research builds on both families of solutions, we detail the techniques used for each in the following paragraphs. We first briefly detail the \textit{Monte-Carlo Tree Search} planning algorithm (\mcts)~\citep{browne2012mcts} used in our first experimental contribution (in chapter~\ref{chap:abig}) and then introduce the deep \rl algorithm used in chapter~\ref{chap:imagine}.

\paragraph{Model-based \rl with \mcts:}

\mcts is a tree-search algorithm that seeks to identify the optimal policy by finding the action with the highest Q-value. To this end, \mcts build an estimate $\hat{Q}(s,a)$ for $a \in \m{A}$ in a given state $s$ and acts greedily with respect to this estimate. Each node of the tree is a state $s$ while edges are the potential actions. The \mcts algorithm grows the tree iteratively using an exploration/exploitation tradeoff to efficiently refine $\hat{Q}$ in promising regions of the \mdp. More specifically, each iteration of the \mcts algorithm contains four steps:
\begin{enumerate}[noitemsep]
	\item \textbf{Selection:} In the selection phase, the \mcts algorithm starts from the root node and uses a tree policy to decide which node to expand. The tree policy is guided by an evaluation function ($UCT$) and stops when a node with remaining actions to explore is reached. 
	\item \textbf{Expansion:} Once a leaf node is reached, a new action $a$ is sampled among the non-explored ones and the corresponding node is computed using the transition function $s' \sim P_E(.|s,a)$
	\item \textbf{Simulation:} From the newly created node corresponding to state $s'$, a simulation policy $\pi_{sim}$ is used to draw a full trajectory (until termination or for a predefined horizon) and compute return $R^{tot}$. $\pi_{sim}$ is often a random policy.
	\item \textbf{Backpropagation:} $R^{tot}$ is backpropagated to the root node as indicated in \fig{fig:mcts_steps}.
\end{enumerate}
For the tree policy evaluation function, we use the Upper Confidence Bound~\citep{auer_finite-time_2002}: $UCT = \frac{1}{k}\sum_{i=0}^{k} R^{tot}_{i} + C\sqrt{\frac{\text{ln}(n)}{k}}$ where $k$ is the number of completed trajectory going through node $s$ and $n$ is the number of iterations. The first term of $UCT$ is an estimation of the expected return while the second term encourages the tree policy to explore unexpanded nodes.
\begin{figure}[!h]
\centering
\includegraphics[width=.95\textwidth]{background/mcts_4_steps.pdf}
\caption{The four steps of an \mcts iteration}
\label{fig:mcts_steps}	
\end{figure}

\paragraph{Model-free \rl with Q-learning:}

Some of the experimental contributions of this research build on the \textit{Deep Deterministic Policy Gradient} (\ddpg) algorithm~\citep{lillicrap2015continuous}. \ddpg derives from \textit{Deep Q-Networks} (\dqn) \citep{mnih2015human} which is itself a deep learning implementation of the standard \textit{Q-learning} algorithm~\citep{watkins_q-learning_1992}.  In this paragraph, we propose to detail the steps that allow building DDPG from Q-learning. 

\textbf{Q-learning} is an \textit{off-policy} \rl algorithm. Off-policy algorithms, in contrast to on-policy algorithms, learn to approximate the action-value $Q^*$ of an optimal policy independently of the policy used for data collection. Q-learning relies on transitions $(s,a,r,s')$ collected by a policy $\pi_c$ interacting with a simulator $P_S$. Assuming that $Q$ is a linear combination of features ($\phi$): $Q(s,a;\theta)=\theta^T\phi(s,a)$, the algorithm iteratively learns to approximate $Q^*$ by minimizing the temporal difference error (TD-error):
\begin{equation}
\m{L}_i=\underset{(s\sim P_S, a\sim\pi_c)}{\mathbb{E}} \left[(y_i - Q(s,a;\theta_i))^2 \right] \text{ with } y_i = \underset{s'\sim P_S}{\mathbb{E}}\left[r +\gamma \max_{a'}Q(s',a';\theta_{i-1})\right] 
\label{eq:loss_td}
\end{equation}
In the original formulation of the Q-learning algorithm by \citet{watkins_q-learning_1992}, they consider a tabular setting and store the Q-values at each iteration in a table ($Q_i[s,a]$) instead of using linear function approximations. The update of the table writes:
\begin{equation}
Q_{i+1}[s,a] \leftarrow Q_i[s,a] + \alpha \left(r + \gamma\max_{a'}Q_i[s',a'] - Q_i[s,a]\right)
\end{equation}


\textbf{\dqn} proposes to represent the action-value function with deep neural networks: $Q(s,a;\theta)$ with parameters $\theta$. The architecture of the network takes a state $s$ as input and outputs the value of each action $Q(s,a) \forall a \in \mathcal{A}$. Thus \dqn only works with discrete action space. When differentiating \eq{eq:loss_td} with respect to the neural network parameters, we get:
\begin{equation}
	\nabla_{\theta_i }\m{L}_i(\theta_i ) = \underset{(s\sim P_S, a\sim\pi_c)}{\mathbb{E}} \left[(y_i - Q(s,a;\theta _i))\nabla_{\theta_i }Q(s,a;\theta _i)\right]
\end{equation}
During differentiation, one has to pay particular attention to freezing the weights of the network when evaluating $y_i$. Deep neural networks are known to exhibit training instabilities. In order to stabilize learning, \citet{mnih2015human} proposed two main innovations: 
\begin{itemize}[noitemsep]
\item \textit{Experience Replay}: The agent uses a replay buffer to store transitions during interactions. During learning, the transitions are then sampled uniformly to perform updates. This enables breaking the correlation between successive transitions and reusing them.
\item \textit{Target network}: A target network is used to compute target $y$. This network is initialized with the actual Q-network ($Q_{targ}(s,a;\theta_{targ})=Q(s,a;\theta$) but updated less frequently than the actual Q-network. Updates are often performed using \textit{Polyak averaging}~\citep{polyak1992}: $\theta_{targ} \leftarrow \rho\theta_{targ} + (1-\rho)\theta$ with $\rho$ being the polyak factor.
\end{itemize}

\textbf{\ddpg} is an adaptation of \dqn to continuous action space. The challenge of dealing with continuous actions is to act greedily with respect to the learned Q-value. i.e. to evaluate $\argmax_a Q(s,a)$. To overcome this, \ddpg concurrently learns a deterministic policy with the Q-function. This policy is a parametrized network $\pi(s;\phi)$ with parameters $\phi$ and is obtained by gradient ascent. Moreover, since $\pi(s;\phi) \approx \argmax_a Q(s,a,\theta)$ it can be injected in \eq{eq:loss_td}. We, therefore, have the two following losses to optimize:
\begin{equation}
\left\{
\begin{split}
	\m{L}_{\pi_\phi} &= \underset{(s\sim P_S)}{\mathbb{E}}\left[Q_\theta(s,\pi_\phi(s))\right] \quad \quad \quad \quad \text{ (Policy loss)}\\
	\m{L}_{Q_\theta} &= \underset{(s\sim P_S,a\sim\pi_c)}{\mathbb{E}}\left[(y- Q_\theta(s,a))^2 \right] \quad \text{(Q-value loss)}\\
	& \text{with } y = \underset{s'\sim P_S}{\mathbb{E}}\left[r +\gamma Q_\theta(s',\pi_\phi)\right]
\end{split}
\right.
\end{equation}
where parameter dependencies have been subscripted.

\paragraph{Other model-free \rl algorithms}

There are numerous algorithms within the field of \drl, including on-policy methos like \textsc{trpo}~\citep{pmlr-v37-schulman15}, \textsc{ppo}~\citep{ppo2017} as well as more advanced off-policy approaches like \textsc{td3}~\citep{pmlr-v80-fujimoto18a} and \textsc{sac}~\citep{pmlr-v80-haarnoja18b}.


\section{Imitation Learning}

\label{sec:background_il}
\paragraph{Problem}

\textit{Imitation Learning} (\il)~\citep{pommerleau1988BC,schaal1996learning,osa2018algorithmic} is a field that considers an agent learning in a \mdp in which the reward function is not explicitly defined, but where the agent can observe demonstrations of the task it is intended to perform. \il is particularly useful in situations where it is difficult for the experimenter to design a task-specific reward function, but demonstrations are available. A classic example from the literature is the application of \il to self-driving cars. It is impractical to specify a reward function for the task of driving as successful drivers constantly adjust their criteria to adapt to the various events that occur on the road. However, there is a vast amount of video footage of people driving that could potentially be utilized by the agent to learn.

\begin{figure}[!h]
\centering
\includegraphics[width=0.3\textwidth]{background/il_interactions.pdf}	
\caption{Interactions in a \il problem. The agent never interacts with the environment during learning but can interact with it to test its behavior (dashed lines).}
\label{fig:il_interacvtions}
\end{figure}

A standard way of formalizing the \il problem is to find a policy that minimizes the divergence between the expert and learner data distribution. Provided a dataset $\m{D}=\{(\tau_i)\}_{i=1}^N$ containing expert trajectories of features $\tau = [\phi_0,\dots, \phi_T]$. If $q_{\pi^*}(\phi)$ is the distribution of features induced by the expert's policy (supposed optimal $\pi^*$) and $p_\pi(\phi)$ is the distribution of features induced by the learners' policy ($\pi$), the goal of \il is to find policy $\hat{\pi}$ such that:
\begin{equation}
\hat{\pi} = \argmin_{\pi}D(q_{\pi^*}(\phi),p_\pi(\phi))
\end{equation}
with $D$ being a measure of differences between probability distributions such as the well-known Kullback-Leibler (KL) divergence.

\paragraph{Behavioral Cloning}

An intuitive way of solving an \il problem is to frame it as a supervised learning setting and do \textit{Behavioral Cloning}(\bc). Given a dataset of trajectories $\m{D}=\{(\tau_i)\}_{i=1}^N$ with  $\tau=[(s_0,a_0) \dots (s_T,a_T)]$, one directly minimizes the cross entropy loss:
\begin{equation}
\m{L}_\pi = -\expect{(s,a)~\sim\m{D}}[\log\pi(s,a)]
\end{equation}
Minimizing this cross-entropy loss is in fact equivalent to minimizing the KL-divergence between the trajectory distribution of the expert $P(\tau|\pi^*)$ and the trajectory distribution of the learner $P(\tau|\pi)$~\citep{ke2020imitation}:
\begin{equation}
D_{KL}(P(\tau|\pi^*),P(\tau|\pi)) = \underset{\tau \in \m{D}}{\sum}P(\tau|\pi^*)\log\left(\frac{P(\tau|\pi^*)}{P(\tau|\pi)}\right)
\end{equation}
%
Injecting the definition of the trajectory distribution of \eq{eq:traj_dist} we get that:
%
\begin{align}
D_{KL}(P(\tau|\pi^*),P(\tau|\pi)) &= \underset{\tau \in \m{D}}{\sum}P(\tau|\pi^*)\log\left(\prod_{t=0}^{T-1}\frac{\pi^*(a_t|s_t)}{\pi(a_t|s_t)}\right) \\
&= \underset{\tau \in \m{D}}{\sum}P(\tau|\pi^*)\sum_{t=0}^{T-1}\left(\log\pi^*(a_t|s_t) - \log\pi(a_t|s_t)\right) \\
&= \expect{(s,a)\sim\m{D}}\left[\log\pi^*(a_t|s_t) - \log\pi(a_t|s_t)\right]
\end{align}
%
We will use behavioral cloning in chapter~\ref{chap:abig}. \bc is a straightforward method for reproducing expert behavior. However, simple \bc only works if the agent operates in the same region of the state space as the states provided in $\m{D}$. Otherwise, the policy of the learner will progressively deviate from this region accumulating errors at each time step. This compounding error is called \textit{distributional mismatch}. One way of addressing it is to iteratively collect new expert data when needed (in the initially uncovered region of the state space)~\citep{dagger2011}. 

Another limitation of \bc is that it is only able to derive an optimal policy from optimal expert trajectories, meaning that the learned policy will not exceed the performance of the expert. In some applications collecting optimal trajectories is not always possible. As a result, some researchers have turned to \textit{Inverse Reinforcement Learning} (\irl) as an alternative approach.

\paragraph{Inverse Reinforcement Learning}

Similar to \rl, \irl can be understood both as a problem and a category of techniques. The \irl problem consists in recovering the reward function of an expert given a dataset of its trajectories~\citep{ng2000irl}. As such \irl algorithmic solutions followed by \rl can form a solution to the \il problem. The combination of \irl followed by \rl is called \textit{Apprenticeship Learning}~\citep{abbeel2004apprenticeship}. As opposed to \bc, apprenticeship learning ensures that the learned policy is bellman consistent (with respect to an underlying learned value-function). As formalized by \citet{klein2011apprenticeship}, there are mainly three categories of strategies to obtain the policy:
\begin{enumerate}[nosep]
\item Feature-expectation-based methods as proposed by \citet{ziebart2008maxentirl} which learn a reward function such that the feature expectation of the optimal policy (according to the learned reward function) is similar to the feature expectation of the expert policy. 
\item Margin-maximization-based methods~\citep{ratlif2006maxmargin}, which formulate \irl as a constrained optimization problem in which the expert's examples have a higher expected cumulative reward than all other policies by a certain margin.
\item Approaches based on the parameterization of the policy by the reward~\citep{neu2007apprenticeship}: If it is assumed that the expert follows a Gibbs policy (or the optimal value function related to the optimized reward function), it is possible to estimate the likelihood of a set of state-action pairs provided by the expert.
\end{enumerate}
%
Recent feature-expectation-based approaches use technics similar to generative adversarial networks (\gan)~\citep{goodfellow2014generative} to imitate complex behavior in high-dimensional environments~\citep{ho2016gail}. Other approaches use ranking of trajectories to reach better-than-demonstrator performances~\citep{brown2020better}. As we do not leverage \irl in our contributions we will not detail these methods (see \citet{arora_survey_2021} for a thorough survey of \irl algorithms).


\section{Muli-Goal Reinforcement Learning}
\label{sec:background_mg_rl}
Standard \rl can be extended to a multi-goal setting. Let us return to the definition of goal by \citet{elliot2008goal} provided in the introduction (\sect{sec:humans_autotelic}):
\begin{quote}
   	``\textit{A goal is a cognitive representation of a future object that the organism is committed to approach or
    avoid} ''.
\end{quote}
\rl algorithms seem, indeed, to be a good fit to train goal-conditioned agents: they train learning agents (\textit{organisms}) to maximize (\textit{approach}) a cumulative (\textit{future}) reward (\textit{object}). In \rl, goals can be seen as a set of \textit{constraints} on one or several consecutive states that the agent seeks to respect. These constraints can be very strict and characterize a single target point in the state space (\eg image-based goals) or a specific sub-space of the state space (\eg target x-y coordinate in a maze, target block positions in manipulation tasks). They can also be more general when expressed by language for example (\eg '\textit{find a red object or a wooden one}'). 

\paragraph{Formal Definition of Goals and Skills}

To represent these goals, \rl agents must be able to 1)~have a compact representation of them and 2)~assess their progress towards it. This is why we propose the following formalization for \rl goals: 

\begin{tcolorbox}
\small
\paragraph{Generalized definition of the goal construct for \rl:}
\begin{itemize}
    \item \gls{Goal}: a $g\,=\,(z_g,\,R_g)$ pair where $z_g$ is a compact \textit{goal parameterization} or \textit{goal embedding} and $R_g$ is a \textit{goal-achievement} function.
    \item \gls{Goal-achievement function}: $R_g(\cdot)\,=\,R_\m{G}(\cdot\mid\,z_g)$ where $R_\m{G}$ is a goal-conditioned reward function.
\end{itemize}
\end{tcolorbox}

The objective of a goal-conditioned agent is to learn a \textit{goal-conditioned policy}: a function that generates the next action given the current state and the goal $a_t \sim \pi(\cdot|s_t,z_g)$. The goal-achievement function and the goal-conditioned policy both assign \textit{meaning} to a goal. The former defines what it means to achieve the goal, it describes how the world looks like when it is achieved. The latter characterizes the process by which this goal can be achieved; what the agent needs to do to achieve it. In this search for the meaning of a goal, the goal embedding can be seen as the map: the agent follows this map and via the two functions above, experiences the meaning of the goal.

\begin{tcolorbox}
\small
\paragraph{Definition}
\begin{itemize}
    \item \gls{Goal-conditioned policy}: a function that generates the next action given the current
state and the goal.
	\item \gls{Skill}:  the association of a goal and a policy to reach it.
\end{itemize}
\end{tcolorbox}

\paragraph{Problem}

By replacing the unique reward function $R$ by the space of reward functions $\m{R}_\m{G}$ in the definition of \mdp of \eq{eq:standard_mdp}, \rl problems can be extended to handle multiple goals: $\m{M}\,=\,\{\m{S},\,\m{A},\,\m{T},\,\rho_0,\,\m{R}_\m{G}\}$. The term \textit{goal} should not be mistaken for the term \textit{task}, which refers to a particular \mdp instance. As a result, \textit{multi-task} \rl refers to \rl algorithms that tackle a set of \mdps that can differ by any of their components (\eg $\m{T}$,\,$R$,\,$\rho_0$, etc.). The \textit{multi-goal} \rl problem can thus be seen as the particular case of the multi-task \rl problem where \mdps differ by their reward functions. In the standard multi-goal \rl problem, the set of goals\,---\,and thus the set of reward functions\,---\,is pre-defined by engineers. As one can observe in \fig{fig:mg_rl_interactions}, the experimenter sets goals to the agent, and provides the associated reward functions. 


\begin{figure}[!h]
\centering
\includegraphics[width=0.3\textwidth]{background/multi_goal_rl_interactions.pdf}	
\caption{Interactions in a multi-goal \rl loop. The experimenter provides goals and their associated rewards to the agent.}
\label{fig:mg_rl_interactions}
\end{figure}


\paragraph{Solutions: Horde, UVFA, and HER}

Goal-conditioned agents see their behavior affected by the goal they pursue. This is formalized via goal-conditioned policies, that is policies that produce actions based on the environment state and the agent's current goal: 
\begin{equation}
\Pi:\m{S}\times\m{Z}_\m{G}\to\m{A}
\end{equation}
where $\m{Z}_\m{G}$ is the space of goal embeddings corresponding to the goal space $\m{G}$~\citep{schaul2015universal}. Note that ensembles of policies can also be formalized this way, via a meta-policy $\Pi$ that retrieves the particular policy from a one-hot goal embedding $z_g$~\citep{kaelbling1993learning,sutton2011horde}.

The idea of using a unique \rl agent to target multiple goals dates back to \citep{kaelbling1993learning}. Later, the \horde architecture proposed to use interaction experience to update one value function per goal, effectively transferring to all goals the knowledge acquired while aiming at a particular one \cite{sutton2011horde}. In these approaches, one policy is trained for each of the goals and the data collected by one can be used to train others.

Building on these early results, \citet{schaul2015universal} introduced \textit{Universal Value Function Approximators} (\uvfa). They proposed to learn a unique goal-conditioned value function and goal-conditioned policy to replace the set of value functions learned in \horde. Using neural networks as function approximators, they showed that \uvfas enable transfer between goals and demonstrate strong generalization to new goals.

The idea of \textit{hindsight learning} further improves knowledge transfer between goals~\citep{kaelbling1993learning,andrychowicz2017hindsight}. Learning by hindsight, agents can reinterpret a past trajectory collected while pursuing a given goal in the light of a new goal. By asking themselves, \textit{what is the goal for which this trajectory is optimal?}, they can use the originally failed trajectory as an informative trajectory to learn about another goal, thus making the most out of every trajectory~\citep{eysenbach2020rewriting}. This ability dramatically increases the sample efficiency of goal-conditioned algorithms and is arguably an important driver of the recent interest in goal-conditioned \rl approaches.

\paragraph{A typology of Goal Representations}

The goal concept and multi-goal \rl will be a central aspect of the autotelic \rl framework that we detail in \sect{sec:self-orga-traj}. Therefore we, here, propose to review the different kinds of goal representations found in the literature. For each category of goal, we detail the form of the goal embedding and the reward function.

\noindent \textbf{Goals as choices between multiple objectives. } Goals can be expressed as a list of different objectives the agent can choose from. This is the case in \citet{oh2017zero,mankowitz2018unicorn,codevilla2018end,chan_actrce_2019}.
\begin{table}[!h]
    \centering
    \small
    \begin{tabularx}{.85\linewidth}{X|X}
    \textit{Goal Embedding}  &    \textit{Reward Function}  \\
    \hline
     $z_g$ are one-hot encodings of the current objective being pursued among the $N$ objectives available. $z_g^i$ is the $i^\text{th}$ one-hot vector: $z_g^i\,=\,(\mathds{1}_{j=i})_{j=[1..N]}$. 
        & The goal-conditioned reward function is a collection of $N$ distinct reward functions $R_\m{G}(\cdot)=R_i(\cdot)$ if $z_g=z_g^i$. 
    \end{tabularx}
\end{table}

\noindent \textbf{Goals as target features of states. } Goals can be expressed as target features of the state the agent desires to achieve.

\begin{table}[!h]
    \centering
    \small
    \begin{tabularx}{.85\linewidth}{X|X}
    \textit{Goal Embedding}  &    \textit{Reward Function}  \\
    \hline
     A state representation function $\varphi$ maps the state space to an embedding space $\m{Z}=\varphi(\m{S})$. Goal embeddings $z_g$ are target points in $\m{Z}$ that the agent should reach.
        & $R_\m{G}$ is based on a distance metric $D$. The reward can be dense: ${R_g=R_\m{G}(s|z_g)=-\alpha\times D(\varphi(s),~z_g)}$, or sparse: ${R_\m{G}(s|z_g)\,=\,1~\text{if}~D(\varphi(s),\,z_g)<\epsilon}$, $0$ otherwise.      
    \end{tabularx}
\end{table}
%
In manipulation tasks, $z_g$ can be target block coordinates \citep{andrychowicz2017hindsight,nair2017overcoming,plappert2018multi,curious,fournier2019clic,blaes2019control,lanier2019curiosity,ding_imitation_2019,li2019towards}. In navigation tasks, $z_g$ can be target agent positions \citep{schaul2015universal,goalgan}. Agent can also target image-based goals. In that case, the state representation function $\varphi$ is usually implemented by a generative model trained on experienced image-based states and goal embeddings can be sampled from the generative model or encoded from real images \citep{zhu2017target,codevilla2018end,nair2018visual,pong2019skew,warde2018unsupervised,florensa2019selfsupervised,venkattaramanujam2019self,pmlr-v100-lynch20a,lynch2020grounding,nair2020contextual,kovac2020grimgep}.

\noindent \textbf{Goals as abstract binary problems.} Some goals cannot be expressed as target state features but can be represented by \textit{binary problems}, where each goal is expressed as a set of constraints on the state that are either verified or not.

\begin{table}[!h]
    \centering
    \small
    \begin{tabularx}{.85\linewidth}{X|X}
    \textit{Goal Embedding}  &    \textit{Reward Function}  \\
    \hline
    $z_g$ can be any expression of the set of constraints
    that the state should respect. \citet{akakzia2020decstr,ecoffet2020first} propose a pre-defined discrete state representation. Another way to express sets of constraints is via language-based predicates
        & The reward function of a binary problem can be viewed as a binary classifier that evaluates whether state $s$ (or trajectory $\tau$) verifies the constraints expressed by the goal semantics (positive reward) or not (null reward)
    \end{tabularx}
\end{table}

When goals are expressed in language, a sentence describes the constraints expressed by the goal, and the state or trajectory either verifies them or does not \citep{Hermann2017,chan2019actrce,Jiang2019,bahdanau2018learning,bahdanau2018systematic,hill2019emergent,ther,imagine,lynch2020grounding}, see \citet{Luketina2019} for a recent review. Language can easily characterize \textit{generic goals} such as ``\textit{grow any blue object}'' (see chapter~\ref{chap:imagine}), \textit{relational goals} like ``\textit{sort objects by size}" \citep{Jiang2019}, ``\textit{put the cylinder in the drawer}" \citep{lynch2020grounding} or even \textit{sequential goals} ``\textit{Open the yellow door after you open a purple door}'' \citep{chevalier-boisvert2018babyai}. When goals can be expressed by language sentences, goal embeddings $z_g$ are usually language embeddings learned jointly with either the policy or the reward function

\noindent \textbf{Goals as a multi-objective balance.} Finally, some goals can be expressed, not as desired regions of the state or trajectory space but as more general objectives that the agent should maximize. In that case, goals can parameterize a particular mixture of multiple objectives that the agent should maximize

\begin{table}[!h]
    \centering
    \small
    \begin{tabularx}{.85\linewidth}{X|X}
    \textit{Goal Embedding}  &    \textit{Reward Function}  \\
    \hline
    $z_g$ are sets of weights balancing the different objectives $z_g\,=\,(\beta_i)_{i=[1..N]}$ where $\beta_i$ is the weights applied to objective $i$ and $N$ is the number of objectives.
        & The reward is expressed as a convex combination of objectives: $R_g(s)\,=\,\sum_{i=1}^N~\beta_g^i R^i(s)$ where $R^i$ is the $i^\text{th}$ of $N$ objectives and $z_g=\beta=\beta_i^g\mid_{i\in[1..N]}$ is the set of weights.
    \end{tabularx}
\end{table}

In \textit{Never Give Up}, for example, \rl agents are trained to maximize a mixture of extrinsic and intrinsic rewards \citep{badia2020never}. The agent can select the mixing parameter $\beta$ that can be viewed as a goal. Building on this approach, \textsc{agent$_{57}$} adds control of the discount factor, effectively controlling the rate at which rewards are discounted as time goes by \citep{badia2020agent57}.


\section{Multi-Agent Reinforcement Learning}
\label{sec:background_marl}
\paragraph{Problem}

Standard \rl can also be extended to scenarios where several agents interact with the environment. For this purpose \mdps are extended to \textit{Markov Games}.
\begin{tcolorbox}
\small
\paragraph{Definition}
\gls{Markov Game} are defined by the following terms:
\begin{equation}
	\m{M}\,=\,\{\m{S},\m{T},\,\rho_0,\,\{\m{O}_i,\m{A}_i,R_i\}_{i=1}^N\}
	\label{eq:markov_game}	
\end{equation}
The first three terms of a Markov Game are the same as those of a \mdp: $\m{S}$ is the state space, $\m{T}$ is the transition function, and $\rho_0$ the initial state distribution. However, each agent (denoted by the index $i$) perceives a different perspective of the state through observation transformation $\m{O}_i$. Agents also have different action spaces $\m{A}_i$ and reward function $R_i$
\end{tcolorbox}

In Multi-Agent Reinforcement learning (\marl), each agent aims at learning a policy that maps their observation $o_i=\m{O}_i(s)$ to actions: $a_i \sim \pi_i(\cdot|o_i)$. Similarly to \rl, each agent aim at maximizing its expected return:
\begin{equation}
	\pi_i^* = \argmax_{\pi_i} \expect{(a_t\sim\pi_i, s_t \sim P_E}\left[\sum_{t=0}^T\gamma^tR_i(\m{O}_i(s_t),a_t)\right]
\end{equation}
A diagram of interaction is provided in \fig{fig:marl_interactions}. The field of \marl considers mainly two types of tasks:
\begin{itemize}[noitemsep]
\item \textit{Cooperative tasks} where the agents pursue the same goal and need to coordinate in order to solve it. Cooperative tasks are usually hard to design and often involve the maximization of a common objective (sometimes at the expense of individual gains). For a review of cooperative \marl see~\citet{OroojlooyJadid2019cooperative}.
\item \textit{Competitive tasks} where the agents pursue non-aligned goals. In these settings agents explicitly aim at maximizing their individual gains. 
\end{itemize}
Among the recent innovations in \marl, \citet{Baker2020Emergent} trained agents to play the hide-and-seek game, \citet{perolat2017commonpool} to solve common-pool resource problems, and more recently \citet{team2021open} trained an agent on a spectrum of cooperative and competitive tasks including cooperative games to find objects, hide and seek or even capture the flag.

\begin{figure}[!h]
\centering
\includegraphics[width=0.5\textwidth]{background/marl_interactions.pdf}	
\caption{Diagram of interactions in a \marl loop. Each agent perceives a (potentially) different perspective of the states provided by the environment. Each agent also has its own action space and is given a (potentially) different reward.}
\label{fig:marl_interactions}
\end{figure}

\paragraph{Solution}

One of the main challenges of multi-agent learning systems is to take into account the non-stationary dynamics caused by the change of state of the agents when they learn. Indeed, an isolated agent of a Markov game does not evolve in a stationary \mdp because all agents are learning, and their behavior will be different during training. For this reason, most of the \marl algorithms rely on the \textit{centralized training, decentralized execution} paradigm. For instance, Muli-Agent Deep Deterministic Policy gradient~\citep{lowe2017multi}, uses a centralized training procedure where all agents can see other agents' observations and actions to learn an action-value function that is then used to optimize decentralized policies that only depends on local observations. As none of our contributions builds on \marl we will not elaborate on other \marl algorithms.






