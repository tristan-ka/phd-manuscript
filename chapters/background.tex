\chapter{Background and Problem Definition}
\label{chap:backgroud}
\adjustmtc
\minitoc

Our contributions bridge standard \ai paradigms and developmental psychology to investigate two fundamental research questions  (1) the language acquisition problem (self-organisation of cultural conventions) and (2) the open-ended skill acquisition problem (self-organisation of trajectories). In this chapter, we will first present standard \ai problems and their associated families of algorithmic solutions before getting into the specifications of the two problems we investigate.

\section{Background: Standard \ai Paradigms}

\label{sec:standard_ai}

\subsection{Reinforcement Learning}

\label{sec:background_rl}

\paragraph{Problem}

In a Reinforcement Learning problem, an agent learns to perform sequences of actions in an environment by maximizing some notion of cumulative reward~\citep{sutton2018reinforcement}.  The agent interacts with the environment in the form of a temporal sequence unfolding from time $t=0$ to time $t=T$, $T$ being the episode horizon and representing the lifetime of the agent (potentially variable or infinite). \rl problems are commonly framed as Markov Decision Processes (\mdps).
\begin{tcolorbox}
\small
\paragraph{Definition}
\gls{Markov Decision Process} (\mdp):
\begin{equation}
	\m{M}\,=\,\{\m{S},\,\m{A},\,\m{T},\,\rho_0,\,R\}
	\label{eq:standard_mdp}	
\end{equation}
where $\m{S}$ and $\m{A}$ are respectively the state and action spaces, $\m{T}$ is the transition function that dictates how actions impact the world (lead to the next state), $\rho_0$ is the initial state distribution and $R$ is the reward function. 
\end{tcolorbox}
%\textbf{Interactions. }  
At the beginning of an episode, the agent starts in the initial state $s_0 \sim \rho_0(\mathcal{S})$. At each time step the agents takes action $a_t \in \mathcal{A}$ and observes the next state $s'=s_{t+1} \in \m{S}$ and the reward $r_{t+1}=R(s_t,a_t)$. A diagram of interaction is given in \fig{fig:rl_interacvtions}.
\begin{figure}[!h]
\centering
\includegraphics[width=0.3\textwidth]{background/rl_interactions.pdf}	
\caption{Interactions in a \rl loop}
\label{fig:rl_interacvtions}
\end{figure}
The transition function $\m{T}$ gives the distribution of the following states from the current state and action: $\m{T}=P_E(.|s, a)$ with $P_E$ being the (potentially stochastic) dynamics of the environment. In an \mdp, the transition function must respect the \textit{Markov property}: a future state ($s'$) must only depend on the current state ($s$) and not on its predecessor, i.e. the transition function is memoryless. 
\begin{equation}
 P_E(s_{t+1}|s_t,a_t) = P_E(s_{t+1}|s_0,\dots,s_t,a_t)	
\end{equation}

In a \rl problem, the behavior of the agent is expressed as a \textit{policy} $\pi : \m{S} \rightarrow \m{A}$ that predicts the next action $a$ based on the current state $s$. This policy can be stochastic ${a_t\sim\pi(.|s_t)}$ or deterministic $a_t=\bar{\pi}(s_t)$. When agents interact in an environment, they produce \textit{trajectories}. A trajectory is a sequence of states and actions $\tau=(s_0,a_0,\dots,s_T,a_T)$. When both the dynamics of the environment and the policy of the agent is stochastic, the probability of a trajectory is:
\begin{equation}
P(\tau|\pi)=\rho_0(s_0)\prod_{t=0}^{T-1} P_E(s_{t+1}|s_t,a_t)\pi(a_t|s_t)	
\label{eq:traj_dist}
\end{equation}
The objective of the agent is to maximize the cumulative reward computed over trajectories ($R^{tot}$). When computing the aggregation of rewards,  we often introduce discounting and give smaller weights to delayed rewards. The return of a trajectory is therefore:
\begin{equation}
R^{tot}(\tau) = \sum_{t=0}^{T} \gamma^{t}R(s_t,a_t)
\end{equation}
with $\gamma \in ]0,1]$ being a constant discount factor. We call the optimal policy $\pi^*$, the behavior that maximizes the expected return:
\begin{equation}
	\pi^*=\argmax_\pi \expect{\tau\sim\pi}[R^{tot}(\tau)] = \argmax_\pi \expect{(a_t\sim\pi, s_t\sim P_E)}\left[\sum_{t=0}^{T} \gamma^{t}R(s_t,a_t)\right]
\end{equation}
The reward function plays therefore a crucial role in a \rl problem as its maximization will directly shape the behavior of the agent.

\paragraph{Value Functions}

Most \rl algorithms rely on the definition of \textit{value} and \textit{action-value} functions:

\begin{tcolorbox}
\small
\paragraph{Definitions}
\begin{itemize}[noitemsep]
	 \item The \gls{value function} $V_\pi(s)$ of a policy $\pi$ gives the expected return of a trajectory starting from $s$ and following $\pi$. 
	 \item The \gls{action-value function} $Q_\pi(s,a)$ is the expected return of the trajectory taking action $a$ from state $s$ before following $\pi$ from the next state $s'$. 
\end{itemize}
\end{tcolorbox}
Action-value functions are powerful because they allow us to instantly assess the quality of a situation without waiting for the end of the trajectory. The value and action-value function obey the Bellman expectation equations~\citep{sutton1998intra}, a recursive definition that states that the value of a certain state (when following policy $\pi$) is equal to the sum of the instantaneous reward and the value from the next state.
\begin{equation}
\left\{
\begin{split}
	V_\pi(s) & = \underset{(a\sim\pi, s'\sim P_E)}{\mathbb{E}}\left[R(s,a) + \gamma V_\pi(s')\right]  \\
	Q_\pi(s,a) &= \underset{s'\sim P_E}{\mathbb{E}}\left[ R(s,a) + \gamma \underset{a' \sim \pi}{\mathbb{E}}[Q_\pi(s',a')]\right]
\end{split}
\right.	
\end{equation}
The value and action-value functions also follow the Bellman optimality equation where expectations over actions are replaced by max operators.
\begin{equation}
\left\{
\begin{split}
	V^*(s) & = \max_a\underset{s'\sim P_E}{\mathbb{E}}\left[R(s,a) + \gamma V^*(s')\right]  \\
	Q^*(s,a) &= \underset{s'\sim P_E}{\mathbb{E}}\left[ R(s,a) + \gamma \max_{a'}[Q^*(s',a')]\right]
\end{split}
\right.	
\end{equation}
Acting greedily with respect to the optimal action-value function gives the optimal policy:
\begin{equation}
\pi^*(s) = \argmax_a Q^*(s,a)	
\end{equation}


Computing $Q^*$ is therefore a way to solve a \rl problem. When agents have access to perfect knowledge of the dynamic of the environment ($P_E$) and when the dimensionality of $\m{S}$ and $\m{A}$ is small, they can do planning to find the optimal action-value function via Dynamic Programming~\citep{bellman1966dynamic} for instance. Planning approaches that leverage the transition function of the environments are called \textit{model-based} \rl algorithms. They are opposed to \textit{model-free} \rl algorithms that do not use $P_E$ but interact directly with a simulator (with transition function $P_S$).

Because the present research builds on both families of solutions, we detail the techniques used for each in the following paragraphs. We first briefly detail the \textit{Monte-Carlo Tree Search} planning algorithm (\mcts)~\citep{browne2012mcts} used in our first experimental contribution (in chapter~\ref{chap:abig}) and then introduce the deep \rl algorithm used in chapter~\ref{chap:imagine}.

\paragraph{Model-based \rl with \mcts:}

\mcts is a tree-search algorithm that seeks to identify the optimal policy by finding the action with the highest Q-value. To this end, \mcts build an estimate $\hat{Q}(s,a)$ for $a \in \m{A}$ in a given state $s$ and acts greedily with respect to this estimate. Each node of the tree is a state $s$ while edges are the potential actions. The \mcts algorithm grows the tree iteratively using an exploration/exploitation tradeoff to efficiently refine $\hat{Q}$ in promising regions of the \mdp. More specifically, each iteration of the \mcts algorithm contains four steps:
\begin{enumerate}[noitemsep]
	\item \textbf{Selection:} In the selection phase, the \mcts algorithm starts from the root node and uses a tree policy to decide which node to expand. The tree policy is guided by an evaluation function ($UCT$) and stops when a node with remaining actions to explore is reached. 
	\item \textbf{Expansion:} Once a leaf node is reached, a new action $a$ is sampled among the non-explored ones and the corresponding node is computed using the transition function $s' \sim P_E(.|s,a)$
	\item \textbf{Simulation:} From the newly created node corresponding to state $s'$, a simulation policy $\pi_{sim}$ is used to draw a full trajectory (until termination or for a predefined horizon) and compute return $R^{tot}$. $\pi_{sim}$ is often a random policy.
	\item \textbf{Backpropagation:} $R^{tot}$ is backpropagated to the root node as indicated in \fig{fig:mcts_steps}.
\end{enumerate}
For the tree policy evaluation function, we use the Upper Confidence Bound~\citep{auer_finite-time_2002}: $UCT = \frac{1}{k}\sum_{i=0}^{k} R^{tot}_{i} + C\sqrt{\frac{\text{ln}(n)}{k}}$ where $k$ is the number of completed trajectory going through node $s$ and $n$ is the number of iterations. The first term of $UCT$ is an estimation of the expected return while the second term encourages the tree policy to explore unexpanded nodes.
\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth]{background/mcts_4_steps.pdf}
\caption{The four steps of an \mcts iteration}
\label{fig:mcts_steps}	
\end{figure}

\paragraph{Model-free \rl with Q-learning:}

Some of the experimental contributions of this research build on the \textit{Deep Deterministic Policy Gradient} (\ddpg) algorithm~\citep{lillicrap2015continuous}. \ddpg derives from \textit{Deep Q-Networks} (\dqn) \citep{mnih2015human} which is itself a deep learning implementation of the standard \textit{Q-learning} algorithm~\citep{watkins_q-learning_1992}.  In this paragraph, we propose to detail the steps that allow building DDPG from Q-learning. 

\textbf{Q-learning} is an \textit{off-policy} \rl algorithm. Off-policy algorithms, in contrast to on-policy algorithms, learn to approximate the action-value $Q^*$ of an optimal policy independently of the policy used for data collection. Q-learning relies on transitions $(s,a,r,s')$ collected by a policy $\pi_c$ interacting with a simulator $P_S$. Assuming that $Q$ is a linear combination of features ($\phi$): $Q(s,a;\theta)=\theta^T\phi(s,a)$, the algorithm iteratively learns to approximate $Q^*$ by minimizing the temporal difference error (TD-error):
\begin{equation}
\m{L}_i=\underset{(s\sim P_S, a\sim\pi_c)}{\mathbb{E}} \left[(y_i - Q(s,a;\theta_i))^2 \right] \text{ with } y_i = \underset{s'\sim P_S}{\mathbb{E}}\left[r +\gamma \max_{a'}Q(s',a';\theta_{i-1})\right] 
\label{eq:loss_td}
\end{equation}
In the original formulation of the Q-learning algorithm by \citet{watkins_q-learning_1992}, they consider a tabular setting and store the Q-values at each iteration in a table ($Q_i[s,a]$) instead of using linear function approximations. The update of the table writes:
\begin{equation}
Q_{i+1}[s,a] \leftarrow Q_i[s,a] + \alpha \left(r + \gamma\max_{a'}Q_i[s',a'] - Q_i[s,a]\right)
\end{equation}


\textbf{\dqn} proposes to represent the action-value function with deep neural networks: $Q(s,a;\theta)$ with parameters $\theta$. The architecture of the network takes a state $s$ as input and outputs the value of each action $Q(s,a) \forall a \in \mathcal{A}$. Thus \dqn only works with discrete action space. When differentiating \eq{eq:loss_td} with respect to the neural network parameters, we get:
\begin{equation}
	\nabla_{\theta_i }\m{L}_i(\theta_i ) = \underset{(s\sim P_S, a\sim\pi_c)}{\mathbb{E}} \left[(y_i - Q(s,a;\theta _i))\nabla_{\theta_i }Q(s,a;\theta _i)\right]
\end{equation}
During differentiation, one has to pay particular attention to freezing the weights of the network when evaluating $y_i$. Deep neural networks are known to exhibit training instabilities. In order to stabilize learning, \citet{mnih2015human} proposed two main innovations: 
\begin{itemize}[noitemsep]
\item \textit{Experience Replay}: The agent uses a replay buffer to store transitions during interactions. During learning, the transitions are then sampled uniformly to perform updates. This enables breaking the correlation between successive transitions and reusing them.
\item \textit{Target network}: A target network is used to compute target $y$. This network is initialized with the actual Q-network ($Q_{targ}(s,a;\theta_{targ})=Q(s,a;\theta$) but updated less frequently than the actual Q-network. Updates are often performed using \textit{Polyak averaging}~\citep{polyak1992}: $\theta_{targ} \leftarrow \rho\theta_{targ} + (1-\rho)\theta$ with $\rho$ being the polyak factor.
\end{itemize}

\textbf{\ddpg} is an adaptation of \dqn to continuous action space. The challenge of dealing with continuous actions is to act greedily with respect to the learned Q-value. i.e. to evaluate $\argmax_a Q(s,a)$. To overcome this, \ddpg concurrently learns a deterministic policy with the Q-function. This policy is a parametrized network $\pi(s;\phi)$ with parameters $\phi$ and is obtained by gradient ascent. Moreover, since $\pi(s;\phi) \approx \argmax_a Q(s,a,\theta)$ it can be injected in \eq{eq:loss_td}. We, therefore, have the two following losses to optimize:
\begin{equation}
\left\{
\begin{split}
	\m{L}_{\pi_\phi} &= \underset{(s\sim P_S)}{\mathbb{E}}\left[Q_\theta(s,\pi_\phi(s))\right] \quad \quad \quad \quad \text{ (Policy loss)}\\
	\m{L}_{Q_\theta} &= \underset{(s\sim P_S,a\sim\pi_c)}{\mathbb{E}}\left[(y- Q_\theta(s,a))^2 \right] \quad \text{(Q-value loss)}\\
	& \text{with } y = \underset{s'\sim P_S}{\mathbb{E}}\left[r +\gamma Q_\theta(s',\pi_\phi)\right]
\end{split}
\right.
\end{equation}
where parameter dependencies have been subscripted.

\paragraph{Other model-free \rl algorithms}

There are numerous algorithms within the field of \drl, including on-policy methos like \textsc{trpo}~\citep{pmlr-v37-schulman15}, \textsc{ppo}~\citep{ppo2017} as well as more advanced off-policy approaches like \textsc{td3}~\citep{pmlr-v80-fujimoto18a} and \textsc{sac}~\citep{pmlr-v80-haarnoja18b}.


\subsection{Imitation Learning}

\paragraph{Problem}

\textit{Imitation Learning} (\il)~\citep{pommerleau1988BC,schaal1996learning,osa2018algorithmic} is a field that considers an agent learning in a \mdp in which the reward function is not explicitly defined, but where the agent can observe demonstrations of the task it is intended to perform. \il is particularly useful in situations where it is difficult for the experimenter to design a task-specific reward function, but demonstrations are available. A classic example from the literature is the application of \il to self-driving cars. It is impractical to specify a reward function for the task of driving as successful drivers constantly adjust their criteria to adapt to the various events that occur on the road. However, there is a vast amount of video footage of people driving that could potentially be utilized by the agent to learn.

\begin{figure}[!h]
\centering
\includegraphics[width=0.3\textwidth]{background/il_interactions.pdf}	
\caption{Interactions in a \il problem. The agent never interacts with the environment during learning but can interact with it to test its behavior (dashed lines).}
\label{fig:il_interacvtions}
\end{figure}

A standard way of formalizing the \il problem is to find a policy that minimizes the divergence between the expert and learner data distribution. Provided a dataset $\m{D}=\{(\tau_i)\}_{i=1}^N$ containing expert trajectories of features $\tau = [\phi_0,\dots, \phi_T]$. If $q_{\pi^*}(\phi)$ is the distribution of features induced by the expert's policy (supposed optimal $\pi^*$) and $p_\pi(\phi)$ is the distribution of features induced by the learners' policy ($\pi$), the goal of \il is to find policy $\hat{\pi}$ such that:
\begin{equation}
\hat{\pi} = \argmin_{\pi}D(q_{\pi^*}(\phi),p_\pi(\phi))
\end{equation}
with $D$ being a measure of differences between probability distributions such as the well-known Kullback-Leibler (KL) divergence.

\paragraph{Behavioral Cloning}

An intuitive way of solving an \il problem is to frame it as a supervised learning setting and do \textit{Behavioral Cloning}(\bc). Given a dataset of trajectories $\m{D}=\{(\tau_i)\}_{i=1}^N$ with  $\tau=[(s_0,a_0) \dots (s_T,a_T)]$, one directly minimizes the cross entropy loss:
\begin{equation}
\m{L}_\pi = -\expect{(s,a)~\sim\m{D}}[\log\pi(s,a)]
\end{equation}
Minimizing this cross-entropy loss is in fact equivalent to minimizing the KL-divergence between the trajectory distribution of the expert $P(\tau|\pi^*)$ and the trajectory distribution of the learner $P(\tau|\pi)$~\citep{ke2020imitation}:
\begin{equation}
D_{KL}(P(\tau|\pi^*),P(\tau|\pi)) = \underset{\tau \in \m{D}}{\sum}P(\tau|\pi^*)\log\left(\frac{P(\tau|\pi^*)}{P(\tau|\pi)}\right)
\end{equation}
%
Injecting the definition of the trajectory distribution of \eq{eq:traj_dist} we get that:
%
\begin{align}
D_{KL}(P(\tau|\pi^*),P(\tau|\pi)) &= \underset{\tau \in \m{D}}{\sum}P(\tau|\pi^*)\log\left(\prod_{t=0}^{T-1}\frac{\pi^*(a_t|s_t)}{\pi(a_t|s_t)}\right) \\
&= \underset{\tau \in \m{D}}{\sum}P(\tau|\pi^*)\sum_{t=0}^{T-1}\left(\log\pi^*(a_t|s_t) - \log\pi(a_t|s_t)\right) \\
&= \expect{(s,a)\sim\m{D}}\left[\log\pi^*(a_t|s_t) - \log\pi(a_t|s_t)\right]
\end{align}
%
We will use behavioral cloning in chapter~\ref{chap:abig}. \bc is a straightforward method for reproducing expert behavior. However, simple \bc only works if the agent operates in the same region of the state space as the states provided in $\m{D}$. Otherwise, the policy of the learner will progressively deviate from this region accumulating errors at each time step. This compounding error is called \textit{distributional mismatch}. One way of addressing it is to iteratively collect new expert data when needed (in the initially uncovered region of the state space)~\citep{dagger2011}. 

Another limitation of \bc is that it is only able to derive an optimal policy from optimal expert trajectories, meaning that the learned policy will not exceed the performance of the expert. In some applications collecting optimal trajectories is not always possible. As a result, some researchers have turned to \textit{Inverse Reinforcement Learning} (\irl) as an alternative approach.

\paragraph{Inverse Reinforcement Learning}

Similar to \rl, \irl can be understood both as a problem and a category of techniques. The \irl problem consists in recovering the reward function of an expert given a dataset of its trajectories~\citep{ng2000irl}. As such \irl algorithmic solutions followed by \rl can form a solution to the \il problem. The combination of \irl followed by \rl is called \textit{Apprenticeship Learning}~\citep{abbeel2004apprenticeship}. As opposed to \bc, apprenticeship learning ensures that the learned policy is bellman consistent (with respect to an underlying learned value-function). As formalized by \citet{klein2011apprenticeship}, there are mainly three categories of strategies to obtain the policy:
\begin{enumerate}[noitemsep]
\item Feature-expectation-based methods as proposed by \citet{ziebart2008maxentirl} which learn a reward function such that the feature expectation of the optimal policy (according to the learned reward function) is similar to the feature expectation of the expert policy. 
\item Margin-maximization-based methods~\citep{ratlif2006maxmargin}, which formulate \irl as a constrained optimization problem in which the expert's examples have a higher expected cumulative reward than all other policies by a certain margin.
\item Approaches based on the parameterization of the policy by the reward~\citep{neu2007apprenticeship}: If it is assumed that the expert follows a Gibbs policy (or the optimal value function related to the optimized reward function), it is possible to estimate the likelihood of a set of state-action pairs provided by the expert.
\end{enumerate}
%
Recent feature-expectation-based approaches use technics similar to generative adversarial networks (\gan)~\citep{goodfellow2014generative} to imitate complex behavior in high-dimensional environments~\citep{ho2016gail}. Other approaches use ranking of trajectories to reach better-than-demonstrator performances~\citep{brown2020better}. As we do not leverage \irl in our contributions we will not detail these methods (see \citet{arora_survey_2021} for a thorough survey of \irl algorithms).


\subsection{Muli-Goal Reinforcement Learning}

Standard \rl can be extended to a multi-goal setting. Let us return to the definition of goal by \citet{elliot2008goal} provided in the introduction (\sect{sec:humans_autotelic}):
\begin{quote}
   	``\textit{A goal is a cognitive representation of a future object that the organism is committed to approach or
    avoid} ''.
\end{quote}
\rl algorithms seem, indeed, to be a good fit to train goal-conditioned agents: they train learning agents (\textit{organisms}) to maximize (\textit{approach}) a cumulative (\textit{future}) reward (\textit{object}). In \rl, goals can be seen as a set of \textit{constraints} on one or several consecutive states that the agent seeks to respect. These constraints can be very strict and characterize a single target point in the state space (\eg image-based goals) or a specific sub-space of the state space (\eg target x-y coordinate in a maze, target block positions in manipulation tasks). They can also be more general when expressed by language for example (\eg '\textit{find a red object or a wooden one}'). 

\paragraph{Formal Definition of Goals and Skills}

To represent these goals, \rl agents must be able to 1)~have a compact representation of them and 2)~assess their progress towards it. This is why we propose the following formalization for \rl goals: 

\begin{tcolorbox}
\small
\paragraph{Generalized definition of the goal construct for \rl:}
\begin{itemize}
    \item \gls{Goal}: a $g\,=\,(z_g,\,R_g)$ pair where $z_g$ is a compact \textit{goal parameterization} or \textit{goal embedding} and $R_g$ is a \textit{goal-achievement} function.
    \item \gls{Goal-achievement function}: $R_g(\cdot)\,=\,R_\m{G}(\cdot\mid\,z_g)$ where $R_\m{G}$ is a goal-conditioned reward function.
\end{itemize}
\end{tcolorbox}

The objective of a goal-conditioned agent is to learn a \textit{goal-conditioned policy}: a function that generates the next action given the current state and the goal $a_t \sim \pi(\cdot|s_t,z_g)$. The goal-achievement function and the goal-conditioned policy both assign \textit{meaning} to a goal. The former defines what it means to achieve the goal, it describes how the world looks like when it is achieved. The latter characterizes the process by which this goal can be achieved; what the agent needs to do to achieve it. In this search for the meaning of a goal, the goal embedding can be seen as the map: the agent follows this map and via the two functions above, experiences the meaning of the goal.

\begin{tcolorbox}
\small
\paragraph{Definition}
\begin{itemize}
    \item \gls{Goal-conditioned policy}: a function that generates the next action given the current
state and the goal.
	\item \gls{Skill}:  the association of a goal and a policy to reach it.
\end{itemize}
\end{tcolorbox}

\paragraph{Problem}

By replacing the unique reward function $R$ by the space of reward functions $\m{R}_\m{G}$ in the definition of \mdp of \eq{eq:standard_mdp}, \rl problems can be extended to handle multiple goals: $\m{M}\,=\,\{\m{S},\,\m{A},\,\m{T},\,\rho_0,\,\m{R}_\m{G}\}$. The term \textit{goal} should not be mistaken for the term \textit{task}, which refers to a particular \mdp instance. As a result, \textit{multi-task} \rl refers to \rl algorithms that tackle a set of \mdps that can differ by any of their components (\eg $\m{T}$,\,$R$,\,$\rho_0$, etc.). The \textit{multi-goal} \rl problem can thus be seen as the particular case of the multi-task \rl problem where \mdps differ by their reward functions. In the standard multi-goal \rl problem, the set of goals\,---\,and thus the set of reward functions\,---\,is pre-defined by engineers. As one can observe in \fig{fig:mg_rl_interactions}, the experimenter sets goals to the agent, and provides the associated reward functions. 


\begin{figure}[!h]
\centering
\includegraphics[width=0.3\textwidth]{background/multi_goal_rl_interactions.pdf}	
\caption{Interactions in a multi-goal \rl loop. The experimenter provides goals and their associated rewards to the agent.}
\label{fig:mg_rl_interactions}
\end{figure}


\paragraph{Solutions: Horde, UVFA, and HER}

Goal-conditioned agents see their behavior affected by the goal they pursue. This is formalized via goal-conditioned policies, that is policies that produce actions based on the environment state and the agent's current goal: 
\begin{equation}
\Pi:\m{S}\times\m{Z}_\m{G}\to\m{A}
\end{equation}
where $\m{Z}_\m{G}$ is the space of goal embeddings corresponding to the goal space $\m{G}$~\citep{schaul2015universal}. Note that ensembles of policies can also be formalized this way, via a meta-policy $\Pi$ that retrieves the particular policy from a one-hot goal embedding $z_g$~\citep{kaelbling1993learning,sutton2011horde}.

The idea of using a unique \rl agent to target multiple goals dates back to \citep{kaelbling1993learning}. Later, the \horde architecture proposed to use interaction experience to update one value function per goal, effectively transferring to all goals the knowledge acquired while aiming at a particular one \cite{sutton2011horde}. In these approaches, one policy is trained for each of the goals and the data collected by one can be used to train others.

Building on these early results, \citet{schaul2015universal} introduced \textit{Universal Value Function Approximators} (\uvfa). They proposed to learn a unique goal-conditioned value function and goal-conditioned policy to replace the set of value functions learned in \horde. Using neural networks as function approximators, they showed that \uvfas enable transfer between goals and demonstrate strong generalization to new goals.

The idea of \textit{hindsight learning} further improves knowledge transfer between goals~\citep{kaelbling1993learning,andrychowicz2017hindsight}. Learning by hindsight, agents can reinterpret a past trajectory collected while pursuing a given goal in the light of a new goal. By asking themselves, \textit{what is the goal for which this trajectory is optimal?}, they can use the originally failed trajectory as an informative trajectory to learn about another goal, thus making the most out of every trajectory~\citep{eysenbach2020rewriting}. This ability dramatically increases the sample efficiency of goal-conditioned algorithms and is arguably an important driver of the recent interest in goal-conditioned \rl approaches.


\subsection{Multi-Agent Reinforcement Learning}

\paragraph{Problem}

Standard \rl can also be extended to scenarios where several agents interact with the environment. For this purpose \mdps are extended to \textit{Markov Games}.
\begin{tcolorbox}
\small
\paragraph{Definition}
\gls{Markov Game} are defined by the following terms:
\begin{equation}
	\m{M}\,=\,\{\m{S},\m{T},\,\rho_0,\,\{\m{O}_i,\m{A}_i,R_i\}_{i=1}^N\}
	\label{eq:standard_mdp}	
\end{equation}
The first three terms of a Markov Game are the same as those of a \mdp: $\m{S}$ is the state space, $\m{T}$ is the transition function, and $\rho_0$ the initial state distribution. However, each agent (denoted by the index $i$) perceives a different perspective of the state through observation transformation $\m{O}_i$. Agents also have different action spaces $\m{A}_i$ and reward function $R_i$
\end{tcolorbox}

In Multi-Agent Reinforcement learning (\marl), each agent aims at learning a policy that maps their observation $o_i=\m{O}_i(s)$ to actions: $a_i \sim \pi_i(\cdot|o_i)$. Similarly to \rl, each agent aim at maximizing its expected return:
\begin{equation}
	\pi_i^* = \argmax_{\pi_i} \expect{(a_t\sim\pi_i, s_t \sim P_E}\left[\sum_{t=0}^T\gamma^tR_i(\m{O}_i(s_t),a_t)\right]
\end{equation}
A diagram of interaction is provided in \fig{fig:marl_interactions}. The field of \marl considers mainly two types of tasks:
\begin{itemize}[noitemsep]
\item \textit{Cooperative tasks} where the agents pursue the same goal and need to coordinate in order to solve it. Cooperative tasks are usually hard to design and often involve the maximization of a common objective (sometimes at the expense of individual gains). For a review of cooperative \marl see~\citet{OroojlooyJadid2019cooperative}.
\item \textit{Competitive tasks} where the agents pursue non-aligned goals. In these settings agents explicitly aim at maximizing their individual gains. 
\end{itemize}
Among the recent innovations in \marl, \citet{Baker2020Emergent} trained agents to play the hide-and-seek game, \citet{perolat2017commonpool} to solve common-pool resource problems, and more recently \citet{team2021open} trained an agent on a spectrum of cooperative and competitive tasks including cooperative games to find objects, hide and seek or even capture the flag.

\begin{figure}[!h]
\centering
\includegraphics[width=0.5\textwidth]{background/marl_interactions.pdf}	
\caption{Diagram of interactions in a \marl loop. Each agent perceives a (potentially) different perspective of the states provided by the environment. Each agent also has its own action space and is given a (potentially) different reward.}
\label{fig:marl_interactions}
\end{figure}

\paragraph{Solution}

One of the main challenges of multi-agent learning systems is to take into account the non-stationary dynamics caused by the change of state of the agents when they learn. Indeed, an isolated agent of a Markov game does not evolve in a stationary \mdp because all agents are learning, and their behavior will be different during training. For this reason, most of the \marl algorithms rely on the \textit{centralized training, decentralized execution} paradigm. For instance, Muli-Agent Deep Deterministic Policy gradient~\citep{lowe2017multi}, uses a centralized training procedure where all agents can see other agents' observations and actions to learn an action-value function that is then used to optimize decentralized policies that only depends on local observations. As none of our contributions builds on \marl we will not elaborate on other \marl algorithms.

%\newpage

\section{Problem Definition: Developmental AI}

The present research expands upon the standard \ai methods presented in previous section, with the aim of investigating fundamental inquiries within the domain of \gls{Developmental Artificial Intelligence}. Developmental \ai is a multidisciplinary field that integrates principles from artificial intelligence, developmental psychology, and neuroscience to simulate and analyze the cognitive mechanisms of artificial agents. While standard \ai paradigms are structured around precise and formal problems addresed by algorithmic contributions, developmental \ai is concerned with modelling the cognitive, sensorimotor and social development of agents. In this research the specific questions that we investigate are: 1) How do cultural conventions emerge through interactions among agents in social contexts? 2) How can embodied agents utilize cultural conventions to acquire new skills? These questions can be approached through the lens of self-organization theory. Specifically, this study will examine: 1) the self-organization of conventions, or language, among agents, and 2) the role of cultural conventions in the self-organization of agents' developmental trajectories. Before introducing these two questions we will therefore define the concept of self-organization and present the challenges inherent in the field of developmental \ai.

\paragraph{Self-organization}

Self-organization is a term now used in a variety of sciences that can be described with the following definition:
\begin{tcolorbox}
\small
\paragraph{Definition}
\gls{Self-organization} is a process by which spontaneously ordered patterns and structures emerge from the interactions of the many constituents of a system without the need for central control or external guidance. Crucially, the emergent global structure of self-organizing systems cannot be explained by the properties of its constituents and must be analyzed as a whole.
\end{tcolorbox}

The notion of \textit{emerging order} draws its origin from the study of chaos and was originally used to describe thermodynamical systems that spontaneously organize themself from complex chaotic interactions. The theory of self-organization was formalized by cybernetician \citet{ashby1962self}. Borrowing concepts from dynamical system theory, he stated that any complex dynamical systems organize themselves around specific 'attractors' within a vast landscape of possible states. These attractors are stable equilibrium points and may be multiple for a given system. An intuitive explanation of attractors, proposed by \citet{dilts1995nlp}, is given in \fig{fig:youngoldlady}. It illustrates how our complex perception system can fall into different attractors when presented with an illusion. In one case, we see a young woman wearing a necklace looking up to the left, in the other we perceive an old woman leaning slightly forward. This illustration also demonstrates that certain attractors may be difficult to reach. Indeed, when looking at \fig{fig:youngoldlady} we often rapidly converge to one attractor and have difficulty escaping it to organize our perception around the other. Finding an attractor requires exploring the landscape of possible states. For this, noise and stochasticity can help as described by \citet{vonFoerster2003}:
\begin{quote}
   	``\textit{I think it is favorable to have some noise in the system. If a system is going to freeze into a particular state, it is inadaptable and this final state may be altogether wrong. It will be incapable of adjusting itself to something that is a more appropriate situation.}''
\end{quote}
\begin{figure}[!h]
\centering
\begin{tabular}{cc}
\includegraphics[width=.2\textwidth]{background/Youngoldwoman.jpg}	& \includegraphics[width=.45\textwidth]{background/stability_plot_yougoldwoman.pdf} \\
(a) & (b)
\end{tabular}
\caption{(a) My Wife and My Mother-In-Law, by the cartoonist W. E. Hill, 1915 (b) Stability plots illustrating the two attractors of the cartoon as proposed by~\citet{dilts1995nlp}}
\label{fig:youngoldlady}
\end{figure}

As displayed in \fig{fig:example_self_org}, nature is full of examples of self-organizing systems. Physical systems exhibit self-organizing behavior through the formation of patterns, such as the longitudinal stripes of sand dunes or the crystalline structure of snowflakes. Similarly, biological systems display self-organizing behaviors\citep{camazine2001self-organization}, as exhibited in the social organization of fish schools and insect swarms, as well as in their ability to collectively adapt to and modify their environment when termites construct mounds and bees build their hives.
%
\begin{figure}[!h]
\small
\centering
\begin{tabular}{c}
	(a) Physical self-organizing systems\\
	\includegraphics[width=.6\textwidth]{background/self_organizing_physical.pdf}\\
	(b)Biological self-organizing systems\\
	\includegraphics[width=.6\textwidth]{background/self_organizing_biological.pdf}
\end{tabular}
\caption{Example of self-organizing systems (a) sand dunes in namibia and cristal structure of a snow ice; and (b) a bee comp and a fish school. Images are royalty-free and obtained from \url{pixabay.com}}
\label{fig:example_self_org}	
\end{figure}

Self-organization also enable creative technical innovation such as the development of self-organizing traffic lights~\citep{ferreira2010self-organized}. These traffic lights are able to adapt to changing traffic conditions through the use of local interactions, rather than relying on communication or external signals. The authors argue that self-organization is particularly well suited for the problem of traffic light regulation because traffic condition change constantly. Thus, the problem at hand is not one of optimization, but rather of adaptation, and self-organization seems to be better equipped to handle this type of challenge.

\paragraph{Self-organization in Developmental \ai}

Developmental \ai problems can be formulated as adaptive problems where one or more agents and the environment are coupled dynamical systems whose interactions are responsible for the agents' behavior~\citep{beer1995dynamical}. The language of dynamical systems and the theory of self-organization can thus help formalize the two fundamental problems of this research.

 First, the problem of the emergence of cultural convention among artificial agents, can be analyzed as the self-organization of a language community~\citep{steels1995selforganizing,oudeyer2005selforganization}. A cultural convention is thus an attractor of a language community: when multiple agents interact, variations of language behaviors are attracted to an equilibrium state because the more members of a community adopt a particular convention, the stronger the convention becomes. We will present several approaches that model language acquisition in \sect{sec:self-orga-lang}. 

Second, the problem of autonomous skill acquisition can be framed as the self-organization of agents' trajectories where agents use internal mechanisms to develop rather than being controlled by hierarchical top-down control~\citep{pfeifer2007robotics}. In this context, agents develop and grow repertoire of skills via internal drivers. These internal drivers, referred to as intrinsic motivations~\citep{oudeyer2005selforganization}, allow agents to self-organize their behavior into developmental trajectories and enable them to acquire increasingly complex skills. We will explore the open-ended skill acquisition problem and present various approaches to it in \sect{sec:self-orga-traj}.


\paragraph{A Multi-faceted Investigation}

\todo{objective is very different from engineering problems in standard ai paradigms. we don't have clear and rigurous problem definition, we rather analyze all the facets of emerging phenomenon. This is why each defintion in the followoing subsection ends with a How to evaluate the system paragraph.}


\todo{example}

\subsection{Self-organisation of Cultural Convention and the Language Acquisition Problem}
\label{sec:self-orga-lang}


\paragraph{Language Games}

\begin{figure}[!h]
\centering
\includegraphics[width=.8\textwidth]{background/language_game_interactions.pdf}	
\caption{Diagram of interactions in a language game}
\label{fig:language_game_interactions}
\end{figure}

steels~\citep{steels2001language}, referential games~\citep{Lewis1969}

\paragraph{Neural Network Communicating Agents}

Work of lazaridou~\citep{lazaridou2017multiagent}

\paragraph{MARL communication}

\begin{figure}[!h]
\centering
\includegraphics[width=.5\textwidth]{background/marl_comm_interactions.pdf}	
\caption{Diagram of interactions in \marl emergence communication setup.}
\label{fig:marl_comm_interactions}
\end{figure}

Sequential decision making problems with multiple agent with explicit channel of communication

Zhang impact of communication bandwith on marl settings~\citep{zhang2013coordinating}, RIAL and DIAL~\citep{foerster2016learning}, scheduling communication~\citep{kim2021communication}

\paragraph{Other related setups (based on fixed protocol of communication)}

Feudal Reinforcement learning? Human robot interactions?

\paragraph{How to evaluate self-organizing communicating agents?}


\subsection{Self-organisation of Trajectories and the Open-ended Skill Acquisition Problem}
\label{sec:self-orga-traj}

\paragraph{Intrinsic Motivations}

\paragraph{Auotelic RL}

\begin{figure}[!h]
\centering
\includegraphics[width=.65\textwidth]{background/autotelic_rl_interactions.pdf}	
\caption{From multi-goal \rl to Autotelic \rl}
\label{fig:autotelic_interactions}
\end{figure}


\paragraph{How to evaluate autotelic agents?}



