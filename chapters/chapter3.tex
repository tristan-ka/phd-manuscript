\chapter{Emergence of Graphical language}
\label{chap:curves}
\minitoc

\begin{abstract}
The framework of Language Games studies the emergence of languages in populations of agents. Recent contributions relying on deep learning methods focused on agents communicating via an idealized communication channel, where utterances produced by a speaker are directly perceived by a listener. This comes in contrast with human communication, which instead relies on a \emph{sensory-motor channel}, where motor commands produced by the speaker (e.g. vocal or gestural articulators) result in sensory effects perceived by the listener (e.g. audio or visual). Here, we investigate if agents can evolve a shared language when they are equipped with a continuous sensory-motor system to produce and perceive signs, e.g. drawings. To this end, we introduce the Graphical Referential Game (\greg) where a speaker must produce a graphical utterance to name a visual referent object consisting of combinations of MNIST digits while a listener has to select the corresponding object among distractor referents, given the produced message. The utterances are drawing images produced using dynamical motor primitives combined with a sketching library. To tackle \greg we present \curves: a multimodal contrastive deep learning mechanism that represents the energy (alignment) between named referents and utterances generated through gradient ascent on the learned energy landscape. We, then, present a set of experiments and metrics based on a systematic compositional dataset to evaluate the resulting  language. We show that our method allows the emergence of a shared, graphical language with compositional properties.
\end{abstract}	

\section{Motivations}

\label{sec:intro_curves}

Understanding the emergence and evolution of human languages is a significant challenge that has involved many fields, from linguistics to developmental cognitive sciences~\citep{CHRISTIANSEN2003300}. Computational experimental semiotics~\citep{galantucci2011experimental} has seen some success in modeling the formation of communication systems in populations of artificial agents~\citep{cangelosi2002simulating, kirby2014iterated}. More specifically, \textit{Language Game} models, such as naming games~\citep{steels2012grounded}, have been used to show how a population of agents can self-organize a culturally shared lexicon without centralized coordination. Given the recent successes of artificial neural networks in solving complex tasks such as image classification~\citep{krizhevsky2012imagenet, he2015delving, he2016deep, dosovitskiy2021image} and natural language understanding~\citep{devlin2019bert, radford2019language, brown2020language}, many works have leveraged them to study the emergence of communication in groups of agents~\citep{lazaridou2020emergent}, mainly using multi-agent deep reinforcement learning and language games~\citep{nguyen2020deep, mordatch2018emergence, lazaridou2018emergence, portelance2021emergence, chaabouni2021communicating}. These advances have made it possible to scale up language game models to environments where linguistic conventions are jointly learned with visual representations of raw image perception, as well as to environments where emergent communication is used as a tool to achieve joint cooperative tasks~\citep{barde2022learning}. 

% Motivations for the experimental setup:
So far, most of these methods have considered only idealized symbolic communication channels based on discrete tokens~\citep{lazaridou2017multiagent,mordatch2018emergence,chaabouni2021communicating} or fixed-size sequences of word tokens~\citep{havrylov2017emergence,portelance2021emergence}. This predefined means of communication is motivated by language's discrete and compositional nature. But how can this specific structure emerge during vocalization or drawing, for instance? Although fundamental in the investigation of the origin of language~\citep{Dessalles2000,cheney2005constraints,oller2019language}, this question seems to be neglected by recent approaches to Language Games~\citep{moulinfrier2020multi}. We, therefore, propose to study how communication could emerge between agents producing and perceiving continuous signals with a constrained \textit{sensory-motor system}. 

\begin{figure}[h!]
\centering 
\includegraphics[width=1\textwidth]{curves/graphical_language_game.pdf}
\caption{\textbf{The Graphical Referential Game:} During an instantiation of the game, the speaker's goal is to produce a motor command $c$ that will yield an utterance $u$ in order to denote a referent $r_S$ sampled from a context $\tilde{R}_S$. Following this step, the listener needs to interpret the utterance in order to guess the referent it denotes among a context $\tilde{R}_L$. The game is a success if the listener and the speaker agree on the referent ($r_L\equiv r_S$).}% At the end of each game, agents swap their roles.}
\label{fig:1}
\end{figure}

Such continuous constrained systems have been used in the cognitive science literature as models of sign production to study the self-organization of speech in artificial systems~\citep{deBoer2000selforganization,oudeyer2006selforganization,MOULINFRIER20155}. 
%They could also be applied to produce gestures as it is deemed a foundational modality in the "gesture-first" hypothesis of language origins\citep{kendon2016reflections}. 
In this paper, we focus on a drawing sensory-motor system producing graphical signs. The sensory-motor system is made of Dynamical Motor Primitives (DMPs)~\citep{schaal2006dynamic} combined with a sketching system~\citep{Mihai2021DifferentiableDA} enabling the conversion of motor commands into images.  Drawing systems have the advantage of producing 2D trajectories interpretable by humans while preserving the non-linear properties of speech models, which were shown to ease the discretization of the produced signals~\citep{STEVENS19893,MOULINFRIER20155}. We introduce the \textit{Graphical Referential Game}: a variation of the original referential game, where a \textit{Speaker} agent (top of Figure~\ref{fig:1}) has to produce a graphical \textit{utterance} given a single target \textit{referent} while a \textit{Listener} agent (bottom of Figure~\ref{fig:1}) has to select an element among a context made of several referents, given the produced utterance (agents alternate their roles).  In this setting, we first investigate whether a population of agents can converge on an efficient communication protocol to solve the graphical language game. Then, we evaluate the coherence and compositional properties of the emergent language, since it is one of the main characteristics of human languages.
%The graphical utterances are generated by a sketching system\cite{Mihai2021DifferentiableDA} which offers fast utterance generation in the form of 2D trajectories interpretable by humans. 

% Motivations for the contrastive algo
Early language game implementations~\citep{steels1995selforganizing,steels2001language} achieve communication convergence by using contrastive methods to update association tables between object referents and utterances. While recent works use deep learning methods to target high-dimensional signals they do not explore contrastive approaches. Instead, they model interactions as a multi-agent reinforcement learning problem where utterances are actions, and agents are optimized with policy gradients, using the outcomes of the games as the reward signal~\citep{lazaridou2017multiagent}. In the meantime, recent models leveraging contrastive multimodal mechanisms such as CLIP~\citep{radford2021learning} have achieved impressive results in modeling associations between images and texts. Combined with efficient generative methods~\citep{ramesh2021zero-shot}, they can compose textual elements that are reflected in image form as the composition of their associated visual concepts. Inspired by these techniques, we propose \curves: Contrastive Utterance-Referent associatiVE Scoring, an algorithmic solution to the graphical referential game. \curves relies on two mechanisms: 1) The contrastive learning of an energy landscape representing the alignment between utterances and referents and 2) the generation of utterances that maximize the energy for a given target referent. We evaluate \curves in two instantiations of the graphical referential game: one with symbolic referents encoded by one-hot vectors and another with visual referents derived from the multiple MNIST digits~\citep{LeCun1998GradientbasedLA}. We show that \curves converges to a shared graphical language that enables a population of agents not only to name complex visual referents but also to name new referent compositions that were never encountered during training.

\paragraph{Scope.} The idea of using a sensory-motor system to study the emergence of forms of combinatoriality in language dates back to methods investigating the origins of digital vocalization systems~\citep{deBoer2000selforganization,oudeyer2005selforganization,zuidema2009evolution}. Such studies were conducted in the context of imitation games at the level of phonemes to observe the formation of speech utterances (syllables, words) that were systematically composed from lower-level meaningless elements (phonemes). This corresponded to the first level of compositionality within the notion of duality of patterning~\citep{hockett1960origin}. Yet, these works did not consider referential games and did not study agents' ability to compose meaningful words to denote referents, i.e. they did not address the second level of the duality of patterning. 

One of the goals of emergent communication research is to develop machines that can interact with humans. As a result, a variety of referential game approaches ensure that the emergent language is as close to natural language. This can be achieved by adding a supervised image captioning objective to encourage agents to use natural language in order to solve their communicative tasks~\citep{havrylov2017emergence,lazaridou2017multiagent}. Other methods use constraints such as memory restrictions~\citep{kottur2017natural} to act as an effective information bottleneck to increase interpretability and compositionality. While we purposefully chose a graphical sensory-motor system to ease the visualization of the emerging language, we do not inject prior knowledge or pressures to facilitate the emergence of an iconic language. Our produced utterances are completely arbitrary. This fundamentally differentiates our work from ~\citet{mihai2021learning} that trains agents to communicate via sketches replicating the visual referents they name. Note also that their drawing setup does not include dynamical motor primitives and utterances are directly optimized in image space allowing gradients to back-propagate from listener to speaker. Finally, they do not consider contrastive learning. To our knowledge, \curves is the first contrastive deep-learning algorithm successfully applied to a referential game.

There is a large body of work exploring the factors that promote
compositionally in emerging languages~\citep{kottur2017natural,li2019ease,rodriguez-luna-etal-2020-internal,Ren2020Compositional,chaabouni2020compositionality,gupta-etal-2020-compositionality}. In this context, a crucial question is how to actually measure it in the first place~\citep{mu2021emergent}. To this end, \citep{choi2018compositional} proposes to measure communicative performances on unseen compositions of known objects as a way to evaluate compositionality. However, it has been shown that a good performance in this test may be achieved without leveraging any actual compositionality in language~\citep{andreas2019measuring, chaabouni2020compositionality}. Thus, others instead compute topographic similarities \citep{brighton2006understanding}, measuring the correlation between distances in the utterance space (distance between signs) and distances in the referents space (such as the cosine similarity between the embeddings of objects)~\citep{lazaridou2018emergence}. In this paper we propose to do both and study 1) the generalization to unseen combinations of abstract features and 2) topographic measures based on the Hausdorff distances between utterances denoting composition and utterances denoting isolated features. 

\textbf{Contributions.} This paper introduces:
\begin{itemize}[noitemsep,topsep=0pt]
    \item The Graphical Referential Game (\greg): a variation of the referential game to study the formation of signs from a graphical sensory-motor system.
    \item \curves: an algorithmic solution to \greg, consisting of a contrastive multimodal encoder coupled with a generative model enabling the emergence of a graphical language.
    \item A systematic study of \curves's generalization performances on compositions of features never seen during training in a simplified control setting and a more perceptually challenging one.
    \item A complementary analysis of the compositionality of the emerging graphical language measuring the Hausdorff distance between utterances denoting compositions and utterances denoting their constituents. 
\end{itemize}


\section{Graphical Referential Games}

\label{sec:prob_def_curves}

\paragraph{Graphical referential game.}

We consider a group of two agents playing a fixed number of referential games, each time alternating their roles (speaker or listener). We will consider two versions of the game: the \textit{discriminative} and the \textit{descriptive}. The \textit{discriminative} game consists in presenting $n$ objects $R$, called referents, to a speaker $S$ and a listener $L$. At the beginning of each game, the target $r^\star \in R$ is assigned to the speaker. Given this target referent $r^\star$, $S$ produces an utterance ($u$) to designate it. Based on the produced utterance $u$, $L$ selects a referent ($\hat{r}$) in $R$. The game outcome $o$ is a success if the selected referent ($\hat{r}\in R$) matches the target $r^\star$. The \textit{descriptive} game is the same, except that the speaker is only provided with $r^\star$ and does not see the context $R$.
 
Additionally, we will consider scenarios, where agents perceive referents from different perspectives. In this case, the speaker perceives the referents $R$ as $\tilde{R}_S$ and its target $r^\star$ as $r^\star_S$. Similarly, the listener perceives the referents $R$ as $\tilde{R}_L$ and selects a referent $\hat{r}$ among it.
 
\paragraph{Sensory-motor drawing system.} 
Utterances are produced by a sensory-motor system ${M: \R^m \rightarrow \mathcal{U} \subset \R^{D\times D}}$ mimicking an arm drawing sketches displayed in Figure~\ref{fig:prob_def}(a). The arm motion is derived from Dynamical Motor Primitives (DMPs)~\citep{schaal2006dynamic}. The DMP is parametrized by a command vector $c\in\R^{20}$. It converts $c$ into a 2-dimensional drawing trajectory $T$ made of 10 coordinates $T=\{v_i\}_{i=0,...,9}$. This trajectory is then fed to a Differentiable Sketching model~\citep{Mihai2021DifferentiableDA} generating an $D\times D$ image (in our implementation, $D=52$).

\begin{figure}[h!]
\centering 
\vspace{-0.3cm}
\begin{tabular}{cc}
\includegraphics[width=0.54\textwidth]{curves/sensory-motor-v2.pdf} &  \includegraphics[width=0.4\textwidth]{curves/referents-v2.pdf}\\
(a) & (b)
\end{tabular}
\vspace{-0.2cm}
\caption{(a) \textbf{Sketching sensory-motor system}: The sensory-motor system imitates a robotic arm drawing a sketch on a 2D plan. DMPs first convert a continuous command $c$ into a sequence of coordinates $T$. This trajectory is then rendered as a $52\times52$ graphical utterance thanks to a differentiable sketching library. (b) \textbf{Referent transformation:} An example of a one-hot context $R$ being transformed into two contexts $\tilde{R}_S$ and $\tilde{R}_L$ by the stochastic transformation $\Phi$. The two contexts are different perspectives of the same objects.}
\vspace{-0.1cm}
\label{fig:prob_def}
\end{figure}

\paragraph{Referents.}
Referents are compositions of orthogonal vector features (one-hot vectors). Given a set of $m$ orthogonal features $F_m$, we define the set of all possible referents as ${\mathcal{R}_m = \{ \textstyle\sum_{f \in S} f | S \subseteq F_m \}}$. The subset of referents made of exactly $k$ features are thus: ${\mathcal{R}^k_m=\{ \textstyle\sum_{f \in S} f | S \subseteq F_m, |S| = k\}}$. In our experiments, we fix $m=5$.

From these orthogonal referents, we propose to generate objects made of digit images sampled from the MNIST dataset~\citep{LeCun1998GradientbasedLA}. More precisely, we define the stochastic mapping $\Phi: \mathcal{R}_m \rightarrow \tilde{\mathcal{R}}_m$ that maps each feature $f \in F_m$ to a digit class in the MNIST dataset. For each feature in a referent, we sample a random instance from the corresponding class and randomly place it on a $4\times4$ grid such that no number overlap. Note that the listener and speaker can perceive different realizations of $
\Phi$, in this case, we say that they see different \textit{perspectives} of the referents.

We use this formalism to instantiate three settings of the Graphical Referential Game (\greg):
\begin{itemize}[noitemsep,topsep=0pt]
    \item \textit{one-hot}: where referents are one-hot vectors $r \in \mathcal{R}_m$.
    \item \textit{visual-shared}: where referents are MNIST digits $r \in \tilde{\mathcal{R}}_m$ and agents share the same perspective: $\tilde{R}_S = \tilde{R}_L$.
    \item \textit{visual-unshared} where referents are MNIST digits $r \in \tilde{\mathcal{R}}_m$ and agents have different perspectives of referents in their contexts $\tilde{R}_S \neq \tilde{R}_L$.
\end{itemize}


\paragraph{Objectives.}
This paper investigates how a group of two agents can agree on a shared compositional language to denote referents, given a continuous sensorimotor system to produce utterances. Beyond the game's success, we evaluate the emerging language along two dimensions. 

\textit{Coherence. } First, we measure the coherence of the emerging lexicon. As utterances are 2-dimensional paths, similarity can be calculated using the Hausdorff distance between sequences of coordinates. The Hausdorff distance $d_H$ is the maximum distance from any coordinate in a trajectory to the closest coordinate in the other:
$d_H(T_1,T_2) = \max\{\sup_{v \in T_1} d(v,T_2), \sup_{v' \in T_2}d(T_1,v') \}$.
In particular, we compute the following metrics.
\begin{itemize}[noitemsep,topsep=0pt]
    \item \textbf{Agents Coherence (A-coherence):} For a given referent $r$ with the same perspective for all agents, measure the mean pairwise similarity between each agent's utterance.
    \item \textbf{Perspective Coherence (P-coherence)}: For a given agent and a given referent $r$, measure the mean pairwise similarity between utterances produced from different perspectives (different instances of $\Phi(r))$.
    \item \textbf{Referents Coherence (R-coherence)}: For a given agent, measure the mean pairwise similarity between utterances produced for different referents.
\end{itemize}

\textit{Compositionality. } The second dimension of our evaluation explores the compositional properties of the emerging language. To this end, we first evaluate the generalization performances of our group to compositional referents never seen during training. More specifically, we train agents on $\mathcal{R}_{\text{train}} = \mathcal{R}_5^1$ (referents made of one feature) and test them on $\mathcal{R}_{\text{test}}=\mathcal{R}_5^2$ (referents made of two features). For visuals about compositional referents, see \ap\ref{sup:ref_comp}. We use the success rate $\textsc{sr}$ to monitor the performances. However, a satisfactory success rate on this testing set does not necessarily imply that the emerging graphical language is in fact compositional. Agents could, for instance, denote compositional referents using newly invented signs.

To complement this analysis, we thus decide to estimate to what extent utterances denoting compositional referents are actually made of utterances denoting their constituents. To this end, we introduce a topographic score $\rho$ that quantifies how an utterances denoting a compositional referent made of feature $i$ and $j$ ($u(r_{ij})$) is actually closer to utterances denoting isolated features $u(r_i)$ or $u(r_j)$ than the utterance naming other compositional referents ($u(r_{xy})$, $x\neq i, y\neq j$). For a detailed derivation of metric $\rho$, see \ap\ref{sup:topo_comp}.

\section{CURVES}


\curves is an energy-based approach that relies on two mechanisms:
\begin{enumerate}[noitemsep,topsep=0pt]
\item  The contrastive learning of an energy landscape $E(r,u)$ is defined as the cosine similarity between utterance and referent embeddings.
\item The generation of an utterance that maximizes the energy for a given target referent $r^\star_S$.
\end{enumerate}
% \begin{enumerate}[noitemsep,topsep=0pt]
% \item  \textbf{a contrastive model} $C: \mathcal{R}_m \times \mathcal{U} \rightarrow \R$ that learns an energy landscape  representing the alignment between utterances and referents.
% \item \textbf{a generative model} $G: \mathcal{R}_m \rightarrow \mathcal{U}$ that produces an utterance that maximizes the energy for a given target referent $r_S$. 
% \end{enumerate}

\textbf{Agents modules and interactions. } Each agent $A\in\{A_1,A_2\}$ trains a contrastive model to learn utterance and referent representations. Its dual encoder $(f_A,g_A)$ maps utterances and referents in a shared $d$-dimensional latent space: $f_A(\cdot,\theta_{fA}): \mathcal{R}_m \rightarrow \mathbb{R}^d$ and $g_A(\cdot,\theta_{gA}): \mathcal{U} \rightarrow  \R^d$ such that $z_{rA}=f_A(r)$ and $z_{uA}=g_A(u)$, as displayed in Figure~\ref{fig:curves_encoders}(a). The energy landscape for each agent is therefore: $E_A(r,u) = \cos(f_A(r),g_A(u))$

A given referential game unfolds as follows. Agents are randomly attributed roles, for instance $A_1$ is the speaker $A_1\leftarrow S$ and $A_2$ is the listener $A_2 \leftarrow L$. The speaker is given a context $\tilde{R}_S$ and a target referent perceived as $r^\star_S$ to produce an utterance $\hat{u}$ intending to maximize $E_S(r^\star_S,u)$. The listener observes $\hat{u}$ and selects referent $\hat{r}$ in context $\tilde{R}_L$ that maximizes $E_L=(r,\hat{u})$:
\begin{equation}
\left\{
\begin{split}
    \hat{u} &\approx u^\star= \underset{u \in \mathcal{U}}{\textrm{argmax }} E_S(r^\star_S,u) \quad \text{(utterance generation from speaker)}\\
    \hat{r} &= \underset{r \in \tilde{R}_L}{\textrm{argmax }} E_L(r,\hat{u}) \quad \text{(referent selection from listener)}
\end{split}
\right.
\label{eq:ut_gen_ref_sel}
\end{equation}
The outcome of the game is then $o = \mathbbm{1}_{[\hat{r}=r^\star]} - b$ where $b$ is a baseline parameter representing the mean success across previous games.  
%\todo{citation for baseline, do we still use it? @yoann}

\begin{figure}[h!]
\centering 
\includegraphics[width=0.95\textwidth]{curves/curves_similarity.pdf}
\caption{(a) \textbf{Agents's dual encoder architecture.} Referents and utterances are mapped to a share latent space. The energy between a referent $r$ and an utterance $u$ is computed as the cosine similarity between their respective embeddings. (b) \textbf{Cosine similarity matrix update from collected samples.} Agents compute the energy for all referents and utterances it collected to form the squared matrix $\Sigma_A$. During contrastive updates agents maximize blue circles and minimize white ones.}
\label{fig:curves_encoders}
\end{figure}

\textbf{Contrastive representation learning in referential games. } 
For a given context $R$, agents are randomly assigned their roles and play $n=|R|$ games. During these $n$ games, roles are fixed and the speaker agent successively selects each referent of the context $\tilde{R}_S$ as the target $r^\star_S$. During interactions, the speaker collects data $\{(r_S^i, u^i, o^i)\}_{i=1,...,n}$ while the listeners observes $\{(u^i, r_L^i)\}_{i=1,...,n}$. From the collected data each agent can compute the squared cosine similarity matrices $\Sigma_A$ whose elements are $(\Sigma_A)_{i,j} = E_A(r_A^i, u^j)$ as shown in Figure~\ref{fig:curves_encoders}(b). Contrastive updates are then performed using the objective $J_A$ that applies \textit{Cross Entropy} ($CE$) on the $i$-th row and $i$-th column of $\Sigma_A$.
\begin{equation}
        J_A(\Sigma_A,i) = [CE((\Sigma_A)_{i,1:n},e_i) + CE((\Sigma_A)_{1:n,i},e_i)] / 2,\\    
\end{equation}
$e_i$ being a one-hot vector of size $n$ with value 1 at index $i$. Depending on the role of the agent, $J_A$ is instantiated either as $J_S$ (speaker) or $J_L$ (listener). Thus, the speaker updates its representation using the outcomes $o_i$ of the games (reinforcing the successful associations while decreasing the unsuccessful ones):
\begin{equation}
        \underset{\theta_{f_S}, \theta_{g_S}}{\textrm{minimize } } \overset{n}{\underset{i = 1}{\sum}} o_i  J_S(\Sigma_S,i)
\end{equation}
On the other hand, the listener needs to make sure that its selection matches the speaker's referent
\citep{steels2015talkingheads} and hence always increases associations (no matter the games' outcomes):
\begin{equation}
    \underset{\theta_{f_L}, \theta_{g_L}}{\textrm{minimize } } \overset{n}{\underset{i = 1}{\sum}} J_L(\Sigma_L,i)
\end{equation}

\textbf{Speaker's utterance optimization. } For the speaker agent, producing an utterance is formalized as maximizing the cosine similarity between the embeddings of a given referent and an utterance produced by our sensory system $u = M(c)$ from motor command $c$. Since $M$ is fully differentiable, we inject the sensory-motor constraint in equation~\ref{eq:ut_gen_ref_sel} and seek for the optimal motor command $c^\star$ using gradient ascent:
\begin{equation}
c^\star = \underset{c \in \mathbb{R}^p}{\textrm{argmax }} E(r^\star_S, M(c))
\label{eq:descri_gen}
\end{equation}
Since the optimization is only performed from the energy between the produced utterance and the target referent $r^\star_S$ we call it \textit{descriptive}. In practice, we do not have any guarantee to reach $c^\star$ and only approach it.

Alternatively, we propose to vary the generation conditions to a \textit{discriminative} scenario where the speaker also perceives the context $\tilde{R}_S$ during production. This is achieved by finding the motor command that minimizes the cross entropy given a target referent $r^\star_S$ and its context $\tilde{R}_S$:
\begin{equation}
    c^\star = \underset{c\in \R ^p}{\textrm{argmin }} CE( \sigma_S, e_{r^\star_S})
    \label{eq:discri_gen}
\end{equation}
%
where $\sigma_S$ is the vector with coordinates $\sigma_{Si} = [E(r^i, M(c))]_{r^i\in \tilde{R}_S}$ and $e_{r^\star_S}$ is the one-hot vector of size $|\tilde{R}_S|$ with value 1 at the position of $r^\star_S$ in $\tilde{R}_S$. This discriminative generation process is only used at test time when investigating \curves's generalization capabilities. 
%

\section{Experiments and Results}

This section focuses first on \curves's training dynamics as agents interact in \greg before showcasing its ability to generalize to compositional referents that were never seen during training. We finally evaluate the compositional structure of the emerging graphical language by providing visuals of utterances and computing topographic scores defined in Section~\ref{sec:prob_def_curves}. Each of these studies is carried out with one-hot, shared-visual, and unshared-visual referents as explained in Section~\ref{sec:prob_def_curves}. Training and testing metrics correspond to the mean and standard deviation computed from training pairs of agents on 10 seeds. 

\textbf{Do agents converge to a shared graphical language? } 

Figure~\ref{fig:conv_coher} displays the training performances of a group of two agents interacting in \greg. For each referent type, the group reaches a perfect success rate of $\textsc{sr}=1$. Moreover, a group starts to converge when inter-agent and inter-perspective coherence distances decrease. This correlation is proof of emergent communication as it indicates that agents start agreeing on signs to denote referents. Finally, the constant (for one-hot referent) and increasing (for visual referents) values of the R-coherence suggest that agents use distinct signs to name referents.

\begin{figure}[h!]
    \centering
    \begin{tabular}{@{}c@{}c@{}c@{}}
    \includegraphics[width=0.33\textwidth]{curves/training/Graph_ONE_HOT.pdf} &  \includegraphics[width=0.33\textwidth]{curves/training/Graph_SHARED.pdf} &
    \includegraphics[width=0.33\textwidth]{curves/training/Graph_UNSHARED.pdf}\\
    (a) & (b) & (c)
    \end{tabular}
    \caption{\textbf{Training success rate and Coherence distances} (a) one-hot referents (b) visual-shared referents (c) visual-unshared referents.}
    \label{fig:conv_coher}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\textwidth]{curves/training/Lexicon_Example.pdf}
    \caption{\textbf{Instance of an emerging lexicon.} Utterances are produced by a group of agents trained with unshared perspectives (1 seed). The perspective for each referent is chosen randomly. }
    \label{fig:lexicon_example}
\end{figure}
An example of an emerging lexicon describing visual referents produced by agents trained on unshared perspectives can be visualized in Figure~\ref{fig:lexicon_example}. Other visualizations for one-hot and shared visual referents are available in \ap\ref{sup:lexicon_one_hot_shared}. We also provide illustrations of P-coherence in \ap\ref{sup:P-coherence}.

\textbf{Are agents able to generalize to compositional referents?} 

Table~\ref{tab:generalization_perf} exposes the generalization performances of group of agents evaluated on referents $r \in \mathcal{R}_5^2$. During an evaluation, the context is exhaustive and contains all the combinations of 2 features: $|R|=10$. We compare the success rates to a \textit{random} baseline where the listener always selects the referent $\hat{r}_L$ randomly no matter the utterance ($\textsc{sr}_{\text{random}}=0.1$). We also introduce a \textit{1-feature} baseline where the speaker produces an utterance $u$ that only denotes one of the two features contained in $r_S^\star$ and the listener randomly selects one of the four combinations containing the communicated feature ($\textsc{sr}_{\text{1-feat}}=0.25$). The success rates for all referent types are significantly higher than the baseline values suggesting that agents are indeed able to communicate about compositional referents. Generalization performances are nearly perfect with one-hot referents but they decrease in visual settings. This performance gap can be explained by the extra difficulty of adding inter-perspective variability to the multi-agent interaction dynamic during the contrastive learning of referent representations. The better success rates obtained in auto-learning (where a single agent plays both the speaker and the listener roles) provided in \ap\ref{sup:auto_social_perf} seem to corroborate this hypothesis. Surprisingly, we observe that success rates for descriptive (Eq.~\ref{eq:descri_gen}) and discriminative (Eq.\ref{eq:discri_gen}) generation are very similar. This suggests that optimizing utterances so as to minimize their energy between non-targeted compositional referents ($r \in R, r \neq r^\star$) does not improve generalization performances.

\begin{table}[!h]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Referents} & \textbf{Descriptive} \textsc{SR} & \textbf{Discriminative} \textsc{SR}  \\
        \hline
         One-hot & $0.99\pm0.01$ & $0.99\pm0.01$ \\
         Visual-shared & $0.57\pm0.04$ & $0.56\pm0.03$ \\
         Visual-unshared & $0.39\pm0.02$ & $0.40\pm0.02$ \\
         \hline
    \end{tabular}
    \caption{\textbf{Generalization performances. } Success rates evaluated on exhaustive context $|R|=10$ with referents $r \in \mathcal{R}_5^2$ for both generative (Eq.~\ref{eq:descri_gen}) and discriminative (Eq.\ref{eq:discri_gen}) utterance generation.}
    \label{tab:generalization_perf}
\end{table}

\textbf{Is the emerging language compositional?} 

To investigate the compositionality of utterances we propose the topographic maps displayed in Figure~\ref{fig:topo_maps}. Each point in a topographic map is an utterance naming a compositional referent $r\in \mathcal{R}_5^2$ and has coordinate $(d_H(u(r_i), \cdot), d_H(u(r_j), \cdot))$. If utterances naming the composition of two features are indeed the compositions of the utterances used to denote each of the isolated features, we expect them to land in the bottom left of the topographic maps. 
%To complement these topographic maps, we, therefore, compute the topographic score $\rho$ that measures how much closer utterances denoting $i$ and $j$ are to utterances $u(r_i)$ and $u(r_j)$ than utterances naming composition of other features.
Figure~\ref{fig:topo_maps} shows that some utterances for compositional referents are indeed close in Haussdorf distance to the utterances denoting the isolated constituent features (Figure~\ref{fig:topo_maps}(b)) but others are not (Figure~\ref{fig:topo_maps}(a)).
\begin{figure}[h!]
    \centering
    \begin{tabular}{cc}
    \includegraphics[width=0.4\textwidth]{curves/unshared_distance_plots/0_d=-0.113.pdf} &  \includegraphics[width=0.4\textwidth]{curves/unshared_distance_plots/9_d=0.203.pdf} \\
    (a) & (b)
    \end{tabular}
    \caption{\textbf{Topographic map examples for a single seed in unshared-visual referents setting} (a) Corresponding to the worst topographic score ${\rho=-0.113}$ (b) Corresponding to the best topographic score $\rho=0.203$. Each utterance names a compositional referent and is colored in blue if it contains feature $i$, orange if it contains feature $j$, green if it contains both, and black if it contains none.}
    \label{fig:topo_maps}
\end{figure}
%


\begin{figure}[!h]
\centering
% \vspace{-1cm}
\includegraphics[width=0.35\textwidth]{curves/compo_m/compo_4_main.pdf}
\caption{\textbf{Matrix of compositions. }Blue frames represent utterances generated for a perspective in $\mathcal{R}_5^1$, other utterance denote the corresponding compositions in $\mathcal{R}_5^2$ }
\label{fig:compo_matrix}
\end{figure}
%
Unfortunately, the feature maps do not allow us to draw strong conclusions about the composition properties of the emerging language. It is hard to tell if agents are indeed composing utterances or if the Haussdorf distance simply does not capture compositionality. This seems to be verified by additional topographic maps provided in \ap\ref{sup:topo_maps}. In particular, the topographic maps for one-hot referents (Figure~\ref{fig:sup_topo_one_hot}) indicate that strong generalization performances can be achieved by producing utterances that are not necessarily close to the isolated feature utterances. This difficulty in evaluating compositionality can be experienced visually thanks to Figure~\ref{fig:compo_matrix} which displays a matrix of composition for unshared-visual referents. More instances of compositions matrices are available in \ap\ref{sup:compo_matr}.


% \newpage
\vspace{.75cm}
\textbf{Are representations compositional?} 

Finally, if compositionality is visually hard to analyze in graphical space, it seems to be much more apparent in the utterance and referent embedding spaces. Figure~\ref{fig:tsne} shows that the embeddings for compositional referents as well as the embedding of the utterances naming them are indeed close to the embeddings of their constituents. This is not surprising since this is the space in which our energy landscape is learned. 



\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\textwidth]{curves/tsnes/descriptive/9-TSNE-R2-R01.pdf}
    \caption{\textbf{T-sne of utterance and referent embeddings. }Embeddings are computed for 100 perspectives of referents. Training conditions are unshared visual referents. Additional t-snes are provided in \ap\ref{sup:tsnes}}
    \label{fig:tsne}
\end{figure}
\section{Discussion and future work}

This work formalizes \greg: a new referential game where two agents must communicate via a continuous sensory-motor system imitating a robotic arm drawing sketches. To tackle \greg, we propose \curves: a contrastive representation learning algorithm inspired by early language game contrastive implementation that scales to high dimensional signals. \curves allows a group of two agents two converge on a shared graphical language in contexts where referents are one-hot vectors or images of MNIST digits. Additionally, the representations that agents learn enable them to communicate about compositional referents never encountered during training. Despite the visualizable nature of graphical signs, compositions of utterances are hard to identify. Our proposed analysis based on the Hausdorff distance could not allow us to draw systematic conclusions. On the other hand, compositions are salient in the space of embeddings.

Future work may look into finding other metrics or evaluation strategies to investigate the composition of utterances in more depth. An analysis of the impact of the sensory-motor constraints on the topology of graphical signs could also provide valuable insight into the factors facilitating the emergence of a compositional graphical language. \curves is agnostic to the modality used to represent utterances. As such, it could tackle other sensory-motor systems.  The central element of \curves lies in the contrastive learning of utterance-referent associations. In our implementation, we optmize utterances by maximizing this energy via gradient ascent. Much like CLIP opened many avenues for multi-modal generation, we could plug in more complex generative strategies such as diffusion models~\citep{Rombach2021HighResolutionIS,Saharia2022PhotorealisticTD}. Finally, more realistic visual referents and the impact of training larger groups of agents on generalization could be investigated in \greg.
