%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% AUTOTELIC AGENTS CHALLENGES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Challenges}
\minitoc
\section{Autotelic Agents Challenges}

\todo{this needs to be pruned and reformulated}
\label{sec:future}
This section discusses open challenges in the quest for autotelic agents tackling the intrinsically motivated skills acquisition problem. 

\subsection{Challenge \#1: Targeting a Greater Diversity of Goals}
\label{sec:future_diversity}
Section~\ref{sec:survey_goal_rep} introduces a typology of goal representations found in the literature. The diversity of goal representations seems however limited, compared to the diversity of goals human target \cite{ram1995goal}.

\paragraph{Time-extended goals.} All \rl approaches reviewed in this paper consider \textit{time-specific} goals, that is, goals whose completion can be assessed from any state $s$. This is due to the Markov property requirement, where the next state and reward need to be a function of the previous state only. \textit{Time-extended} goals\,---\,\ie goals whose completion can be judged by observing a sequence of states (\eg \textit{jump twice})\,---\,can however be considered by adding time-extended features to the state \cite<\eg the difference between the current state and the initial state>{imagine}. To avoid such \textit{ad-hoc} state representations, one could imagine using reward function architectures that incorporate forms of memory such as Recurrent Neural Network (\rnn) architectures \cite{elman} or Transformers \cite{vaswani2017attention}. Although recurrent policies are often used in the literature \cite{chevalier-boisvert2018babyai,hill2019emergent,loynd2019working,goyal2019recurrent}, recurrent reward functions have not been much investigated. Some work \cite{sutton_tdnet_2004,schlegel_general_2021} investigate the benefit of computing relations between value functions when learning predictive representations. \cite{sutton_tdnet_2004} propose to represent the interrelation of predictions in a \textit{TD-network} where nodes are predictions computed from states. The network allows to perform predictions that have complex temporal semantics. \cite{schlegel_general_2021} train a RNN architecture where hidden-states are multi-step predictions. Finally, recent work by \cite{karch2021grounding} show that agents can derive rewards from linguistic descriptions of time-extended behaviors.  Time-extended goals include interactions that span over multiple time steps (\eg \textit{shake the blue ball}) and spatio-temporal references to objects (\eg \textit{get the red ball that was on the left of the sofa yesterday}).


\paragraph{Learning goals.}
\textit{Goal-driven learning} is the idea that humans use \textit{learning goals}, goals about their own learning abilities as a way to simplify the realization of \textit{task goals} \cite{ram1995goal}. Here, we refer to \textit{task goals} as goals that express constraints on the physical state of the agent and/or environment. On the other hand, \textit{learning goals} refer to goals that express constraints on the knowledge of the agent. Although most \rl approaches target task goals, one could envision the use of \textit{learning goals} for \rl agents. 

In a way, learning-progress-based learning is a form of learning goal: as the agent favors regions of the goal space to sample its task goals, it formulates the goal of learning about this specific goal region \cite{baranes2013active,fournier2018accuracy,fournier2019clic,curious,blaes2019control,akakzia2020decstr}.

Embodied Question Answering problems can also be seen as using learning goals. The agent is asked a question (\ie a learning goal) and needs to explore the environment to answer it (acquire new knowledge) \cite{das2018embodied,Yuan_2019}. 

In the future, one could envision agents that set their own learning targets as sub-goals towards the resolution of harder task or learning goals, \eg \textit{I'm going to learn about knitting so I can knit a pullover to my friend for his birthday.}
%\os{Meta-Learning is generally defined as learning to learn. For the agent, learning the reward function or having learning goals and learning to reach them is about learning to learn. Maybe this general statement and a few refs should come earlier.}

\paragraph{Goals as optimization under selected constraints.}
We discussed the representations of goals as a balance between multiple objectives. An extension of this idea is to integrate the selection of constraints on states or trajectories. One might want to maximize a given metric (\eg walking speed), while setting various constraints (\eg maintaining the power consumption below a given threshold or controlling only half of the motors). The agent could explore in the space of constraints, setting constraints to itself, building a curriculum on these, etc. This is partially investigated in \cite{colas2020epidemioptim}, where the agent samples constraint-based goals in the optimization of control strategies to mitigate the economic and health costs in simulated epidemics. This approach, however, only considers constraints on minimal values for the objectives and requires the training of an additional Q-function per constraint.

\paragraph{Meta-diversity of goals.} Finally, autotelic agents should learn to target all these goals within the same run; to transfer their skills and knowledge between different types of goals. For instance, targeting visual goals could help the agent explore the environment and solve learning goals or linguistic goals. As the density of possible goals increases, agents can organize more interesting curricula. They can select goals in easier representation spaces first (\eg sensorimotor spaces), then move on to target more difficult goals (\eg in the visual space), before they can target the more abstract goals (\eg learning goals, abstract linguistic goals). 

This can take the form of goal spaces organized hierarchically at different levels of abstractions. The exploration of such complex goal spaces has been called \textit{meta-diversity}  \cite{etcheverry_hierarchically-organized_2020}. In the outer-loop of the meta-diversity search, one aims at learning a diverse set of outcome/goal representations. In the inner-loop, the exploration mechanism aims at generating a diversity of behaviors in each existing goal space. How to efficiently transfer knowledge and skills between these multi-modal goal spaces and how to efficiently organize goal selection in large multi-modal goal spaces remains an open question.

\subsection{Challenge \#2: Learning to Represent Diverse Goals}
\label{sec:future_imagination_and_composition}
This survey mentioned only a handful of complete autotelic architectures. Indeed, most of the surveyed approach assume pre-existing goal embeddings or reward functions. Among the approaches that learn goal representations autonomously, we find that the learned representations are often restricted to very specific domains. Visual goal-conditioned approaches for example, learn reward functions and goal embeddings but restrict them to the visual space \cite{nair2018visual,nair2020contextual,warde2018unsupervised,venkattaramanujam2019self,pong2019skew,hartikainen2019dynamical}. Empowerment methods, on the other hand, develop skills that maximally cover the state space, often restricted to a few of its dimensions \cite{achiam_variational_2018,eysenbach2018diversity,campos_explore_2020,sharma_dynamics-aware_2020}. 

These methods are limited to learn goal representations within a bounded, pre-defined space: the visual space, or the (sub-) state space. How to autonomously learn to represent the wild diversity of goals surveyed in Section~\ref{sec:survey_goal_rep} and discussed in Challenge \#1 remains an open question.

\subsection{Challenge \#3: Imagining Creative Goals}
Goal sampling methods surveyed in Section~\ref{sec:survey_generation} are all bound to sample goals \textit{within the distribution of known effects}. Indeed, the support of the goals distribution is either pre-defined  \cite<\eg >{schaul2015universal,andrychowicz2017hindsight,curious,li2019towards} or learned using a generative model \cite{goalgan,nair2018visual,nair2020contextual,pong2019skew} trained on previously experienced outcomes. On the other hand, humans can imagine creative goals beyond their past experience which, arguably, powers their exploration of the world. 

In this survey, one approach opened a path in this direction. The \imagine algorithm uses linguistic goal representation learned via social supervision and leverages the compositionality of language to imagine creative goals beyond its past experience \cite{imagine}. This is implemented by a simple mechanism detecting templates in known goals and recombining them to form new ones. This is in line with a recent line of work in developmental psychology arguing that human play might be about practicing to generate plans to solve imaginary problems  \cite{chu2020exploratory}.

Another way to achieve similar outcomes is to compose known goals with Boolean algebras, where new goals can be formed by composing existing atomic goals with negation, conjunction and disjunctions. The logical combinations of atomic goals was investigated in \cite{tasse2020boolean,chitnis2020glib}, and \cite{colas2020language,akakzia2020decstr}. The first approach represents the space of goals as a Boolean algebra, which allows immediate generalization to compositions of goals (\textsc{and}, \textsc{or}, \textsc{not}). The second approach considers using general symbolic and logic languages to express goals, but uses symbolic planning techniques that are not yet fully integrated in the goal-conditioned deep \rl framework. The third and fourth train a generative model of goals conditioned on language inputs. Because it generates discrete goals, it can compose language instructions by composing the finite sets of discrete goals associated to each instruction (\textsc{and} is the intersection, \textsc{or} the union etc). However, these works fall short of exploring the richness of goal compositionality and its various potential forms. \cite{tasse2020boolean} seem to be limited to specific goals as target features, while \cite{akakzia2020decstr} requires discrete goals. Finally, \cite{barreto_option_2019} proposes to target new goals that are represented by linear combination of pseudo-rewards called \textit{cumulants}. They use the option framework and show that an agent that masters a set of options associated with cumulants can generalize to any new behavior induced by a linear combination of those known cumulants. 

\subsection{Challenge \#4: Composing Skills for Better Generalization}

Although this survey focuses on goal-related mechanisms, autotelic agents also need to learn to achieve their goals. Progress in this direction directly relies on progress in standard \rl and goal-conditioned \rl. In particular, autotelic agents would considerably benefit from better generalization and skill composition. Indeed, as the set of goals agents can target grows, it becomes more and more crucial that agents can efficiently transfer knowledge between skills, infer new skills from the ones they already master and compose skills to form more complex ones. Although hierarchical \rl approach learn to compose skills sequentially, concurrent skill composition remains under-explored.


\subsection{Challenge \#6: Leveraging Socio-Cultural Environments}
Decades of research in psychology, philosophy, linguistics and robotics have demonstrated the crucial importance of rich socio-cultural environments in human development \cite{vygotsky_thought_1934,whorf_language_1956,wood_role_1976,rumelhart_sequential_1986,berk_why_1994,clark_being_1998,tomasello_cultural_1999,tomasello_constructing_2009,zlatev_epigenesis_2001,carruthers_modularity_2002,dautenhahn_embodied_2002,lindblom_social_2003,mirolli_towards_2011,lupyan_what_2012}. However, modern \ai may have lost track of these insights. Deep reinforcement learning rarely considers social interactions and, when it does, models them as direct teaching; depriving agents of all autonomy. A recent discussion of this problem and an argument for the need of agents that are both autonomous and teachable can be found in a concurrent work \cite{sigaud2021towards}. As we embed autotelic agents in richer socio-cultural worlds and let them interact with humans, they might start to learn goal representations that are meaningful for us, in our society. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% VYGOTSKIAN AGENTS CHALLENGES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Vygotksian Agents challenges}

\label{sec:challenges}
We identify three main challenges for future research. 

\textbf{Challenge \#1: Immersing autotelic agents in rich socio-cultural worlds.}
To benefit from language, Vygotskian autotelic agents must be immersed into rich socio-cultural worlds close to ours. This will require progress along two dimensions: 1)~increasing the richness of their world and 2)~augmenting their interactivity and teachability.

What do we mean by \textit{rich} worlds? One aspect is the multimodality of perceptions. Beyond its linguistic dimension, culture is indeed multimodal. Socio-cultural interactions are not always linguistic but often non-verbal as they may involve motor, perceptual or emotional dimensions. The second aspect is socio-cultural situatedness: autotelic agents must interact with other agents and with humans. Scaling the richness of these worlds may thus require the involvement of the video game industry, and specialists in complex, realistic multimodal worlds. Human-in-the-loop research will also be required to let humans enter these rich virtual worlds, for instance via virtual reality technology. 

Vygotskian autotelic agents will need to be more interactive and teachable. In a recent paper, Sigaud and colleagues discuss this challenge through a detailed analysis of children's learning abilities and teacher-child interactions.\cite{sigaud_towards_2021} They present a checklist of properties that future Vygotskian autotelic agents must demonstrate to be considered \textit{teachable}. To interact with humans, Vygotskian autotelic agents will also need to target goals in multiple modalities (\eg linguistic, perceptual, emotional) with various levels of abstraction.\cite{colas2021intrinsically} Modular autotelic architectures may be used to that end. By handling multiple goal spaces in parallel, they can leverage cross-domain hindsight learning: using experience collected while aiming at a goal to learn about other goals in other domains.\cite{curious} 

\textbf{Challenge \#2: Enabling artificial mental life with systematic internalized language production}
Only a few approaches internalize language production within agents. So far limited to a few use cases, language production should concern every possible linguistic feedback agents could receive: instructions, corrections, advice, explanations, or cultural artefacts. This internal language production is akin to an artificial \textit{inner speech}, the embryo of \textit{artificial mental life}. Looping back to the constitutive thesis of Carruthers presented in Section~\ref{sec:4views}, inner speech acts as a common currency for inner modules to exchange information (see a recent implementation of this idea\cite{zeng2022socratic}). Combined with world models, inner speech could trigger the simulation of perceptual experience (images, sounds), sensorimotor trajectories, the imagination of possible futures or past memories. Observing these hallucinations, agents could produce new behaviors and new inner speech. This inner loop acts as a mental life that could help agents reason; trigger memories or mnemotechnic representations acting as cognitive aids. As noted by Dove, this account is fully compatible with the embodied hypothesis in cognitive science.\cite{dove_language_2018} Following this hypothesis, thinking and modeling sensorimotor experience are one and the same. Here, language brings another set of inputs and outputs for these simulation models and the simulation of abstract content (words, analogical structures, etc) might offer us the capacity to reason abstractly.  

\textbf{Challenge \#3: Building editable and shareable cultural models with aligned \llms.} 
Large language models encode a lot of information about the human cultures that generated the texts they were trained on \llms,\cite{west_symbolic_2021,schramowski2022large}. They can be viewed as (partial) cultural models: by tapping into them, agents could learn about human cultures. They could learn about foundational human concepts, causality, folk psychology, politeness, ethics and all these physical or cultural information that are the subject of everyday stories: fiction, news, or even simple narratives parents use to explain everyday things to children. 

Such proxies to human cultures provide both opportunities and challenges. Using existing \llms as is, we could for example prompt them to generate new goals for exploration or even full curricula based on descriptions of the agent's current abilities and environmental descriptions. We could use \llms to predict the outcome of the agent's actions given the context and use this to plan in abstract search spaces. We could let agents ask \llms for guidelines only when they cannot solve the problem themselves (active learning), and more generally to augment the world state with commonsense knowledge.

But letting autotelic agents rely on \llms might also bring some downsides. Cultural information, because it biases the search space, may limit exploration and lead to the premature abandonment of promising avenues\cite{bonawitz2011Double} (\eg in astronomy, the cultural support for the geocentric model significantly delayed the acceptance of the heliocentric model). \llms are also known to convey false information and harmful biases, either because they inadequately learned to encode a culture or because they were trained on cultural artefacts which contained such biases.\cite{shah-etal-2020-predictive,pmlr-v139-liang21a,weidinger2021ethical,bender2021dangers} Autotelic agents relying on these models could demonstrate harmful behaviors and contribute to reinforcing stereotypes and inequalities. The use of \llms will thus require advances in bias mitigation strategies\cite{pmlr-v139-liang21a,bender2021dangers} and improved alignment methods to make \llms more reliable, trustworthy and moral. Ideally, we want them to model the natural culture agents will be embedded in with high fidelity and align well with its objectives.

Humans are also biased by the cultural environment they are in. During their education, children are taught to think for themselves and to think critically. Autotelic agents should be taught in the same way. Because they are autonomous embodied machines, they can conduct experiments in the world and empirically test the information they were provided. This physical embodiment is often described as the missing piece for \llms to truly understand the world.\cite{bisk2020experience} Just like human cultural narratives can be shifted by government, policies, advertising, activism, and pop culture, artificial cultural narratives should become more malleable. Autotelic agents must be given the possibility to steer, edit and extend their cultural models (\ie \llms) in light of their embodied experience; to share it and negotiate it with others; \ie to participate in a shared cultural evolution. Reversely, humans that train language/cultural models should pay great care to build and understand the cultural input they provide, just like they pay great care to the education of their children. 

Through this process, agents could learn some of the uniquely human social features described by Tomasello in his recent book\cite{Tomasello+2019}: cooperative thinking, moral identity or social norms. However, a high-quality alignment may require humans to enter the interaction loop, enabling autotelic agents to ground, verify and correct the cultural knowledge they acquired with \llms. Furthermore, mere exposure to culture (either through \llms or direct interaction with humans) might not be sufficient to design truly autonomous social learners. This may require encoding certain mechanisms such as joint intentionality or other collaborative priors inside agents\,---\,the topic of social \rl.\cite{jaques2019social}


\todo{talk about textworld and only textual data; this discussion can help: https://twitter.com/pyoudeyer/status/1613145892026187777}

\textbf{Challenge \#4: Pursuing long-term goals.} Current autotelic agents mostly pursue goals at the timescale of an episode. Humans, on the other hand, can pursue goals they can barely hope to achieve within their lifetime (\eg building an efficient fusion reactor). Because there is an infinity of potential goals and little time to explore them alone, autotelic agents may need cultural models to bias their selection of long-term goals towards more feasible, interesting, or valuable options\,---\,turning an individual exploration into a \textit{population-based exploration}.  Keeping long-term goals in mind will require improvements in architecture's memory systems, but might also benefit from language and culture. Indeed, verbalization is known to increase humans' memory span \cite{elliott_multilab_2021} and writing let us set our goals in stone (from the post-it note reminding you to take the garbage out to the Ten Commandments). Young children progressively become future-oriented as they are taught to project themselves into the future through education, social interactions (\textit{what do you want to do when you grow up?}) and cultural metaphors (\eg the self-made man).\cite{atance2008future} If autotelic agents will need better hierarchical \rl algorithms to achieve long-term goals, they could also leverage cultural artifacts evolved for improved collaboration and long-term planning\,---\,think of roadmaps, organization systems and
project management tools.\cite{carruthers_magic_1998} Because long-term goals are not rewarding, human cultures supply short-term social rewards (good grades in the educational system, money and social recognition in professional careers)\,---\,a form of reward shaping. 


\todo{in the organization of this manuscript we decoupled the self-organization of language and the self-organization of trajectories, an interesting research direction will be to do both at the same time}

